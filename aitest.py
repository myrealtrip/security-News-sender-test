"""
AI ì¤‘ì‹¬ ë³´ì•ˆ ë‰´ìŠ¤ í•„í„°ë§ ë° ìŠ¬ë™ ë°œì†¡ ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” íŠ¹ì§•:
- AIê°€ ì£¼ë„ì ìœ¼ë¡œ ê¸°ì‚¬ë¥¼ íŒë‹¨í•˜ê³  ì„ íƒ
- ì½”ë“œ í•„í„°ë§ ìµœì†Œí™” (ì¤‘ë³µ ì²´í¬ë§Œ)
- AIì˜ decisionì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‹ ë¢°
"""

import os
import json
import time
import re
import hashlib
import requests
import feedparser
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# RSS í”¼ë“œ URL
RSS_URLS = [
    "https://www.boho.or.kr/kr/rss.do?bbsId=B0000133",    
    "https://www.boannews.com/media/news_rss.xml?kind=1",
    "https://www.dailysecu.com/rss/S1N2.xml"
]

STATE_FILE = "state.aitest.json"  # AI í…ŒìŠ¤íŠ¸ìš© ìƒíƒœ íŒŒì¼
SLACK_BOT_TOKEN = os.environ.get("SLACK_BOT_TOKEN", "")
SLACK_CHANNEL = os.environ.get("SLACK_CHANNEL", "")
if not SLACK_BOT_TOKEN or not SLACK_CHANNEL:
    print("âš ï¸  ê²½ê³ : SLACK_BOT_TOKEN ë˜ëŠ” SLACK_CHANNEL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì—ëŠ” ìŠ¬ë™ ë°œì†¡ì´ ê±´ë„ˆëœë‹ˆë‹¤.")

# AI API ì„¤ì •
USE_AI_JUDGMENT = os.environ.get("USE_AI_JUDGMENT", "true").lower() == "true"
AI_PROVIDER_RAW = os.environ.get("AI_PROVIDER", "anthropic")
AI_PROVIDER = AI_PROVIDER_RAW.strip().lower()  # "openai" or "anthropic"

# OpenAI ì„¤ì •
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

# Anthropic (Claude) ì„¤ì •
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
ANTHROPIC_MODEL = os.environ.get("ANTHROPIC_MODEL", "claude-3-5-haiku-20241022")

# AI íŒë‹¨ ê¸°ì¤€ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ
AI_PROMPT_FILE_FIRST_STAGE = os.environ.get("AI_PROMPT_FILE_FIRST_STAGE", "ai_prompt_first_stage.txt")
AI_PROMPT_FILE_SECOND_STAGE = os.environ.get("AI_PROMPT_FILE_SECOND_STAGE", "ai_prompt_second_stage.txt")
# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
AI_PROMPT_FILE = os.environ.get("AI_PROMPT_FILE", "ai_prompt_simple.txt")

HEADERS = {
    "User-Agent": "MyrealtripSecurityBot/1.0"
}

# -----------------------------
# ìƒíƒœ ê´€ë¦¬
# -----------------------------
def load_state():
    """ìƒíƒœ íŒŒì¼ ë¡œë“œ"""
    if not os.path.exists(STATE_FILE):
        return {
            "seen": [],
            "seen_titles": [],
            "seen_original_titles": [],
            "seen_links": [],
            "prompt_hash": None,
            "last_prompt_change": None
        }
    state = json.load(open(STATE_FILE, "r", encoding="utf-8"))
    # ê¸°ì¡´ state íŒŒì¼ì— í•„ë“œê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ˆê¸°í™”
    if "seen_titles" not in state:
        state["seen_titles"] = []
    if "seen_original_titles" not in state:
        state["seen_original_titles"] = []
    if "seen_links" not in state:
        state["seen_links"] = []
    if "prompt_hash" not in state:
        state["prompt_hash"] = None
    if "last_prompt_change" not in state:
        state["last_prompt_change"] = None
    return state

def get_prompt_hash():
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ì˜ í•´ì‹œê°’ ê³„ì‚° (1ì°¨ì™€ 2ì°¨ í”„ë¡¬í”„íŠ¸ ëª¨ë‘ í¬í•¨)"""
    try:
        hash_parts = []
        # 1ì°¨ í”„ë¡¬í”„íŠ¸ í•´ì‹œ
        if os.path.exists(AI_PROMPT_FILE_FIRST_STAGE):
            with open(AI_PROMPT_FILE_FIRST_STAGE, "r", encoding="utf-8") as f:
                content = f.read()
                hash_parts.append(hashlib.md5(content.encode('utf-8')).hexdigest())
        # 2ì°¨ í”„ë¡¬í”„íŠ¸ í•´ì‹œ
        if os.path.exists(AI_PROMPT_FILE_SECOND_STAGE):
            with open(AI_PROMPT_FILE_SECOND_STAGE, "r", encoding="utf-8") as f:
                content = f.read()
                hash_parts.append(hashlib.md5(content.encode('utf-8')).hexdigest())
        # ë‘ í•´ì‹œë¥¼ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ í•´ì‹œ ìƒì„±
        if hash_parts:
            combined = "|".join(hash_parts)
            return hashlib.md5(combined.encode('utf-8')).hexdigest()
    except Exception as e:
        print(f"[WARN] í”„ë¡¬í”„íŠ¸ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
    return None

def check_prompt_changed(state):
    """í”„ë¡¬í”„íŠ¸ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ë³€ê²½ ì‹œ ì•Œë¦¼"""
    current_hash = get_prompt_hash()
    if current_hash is None:
        return False
    
    previous_hash = state.get("prompt_hash")
    if previous_hash is None:
        # ì²˜ìŒ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
        state["prompt_hash"] = current_hash
        return False
    
    if current_hash != previous_hash:
        print(f"[INFO] âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"[INFO] ì´ì „ í•´ì‹œ: {previous_hash[:8]}... â†’ ìƒˆ í•´ì‹œ: {current_hash[:8]}...")
        print(f"[INFO] ìƒˆë¡œìš´ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì‚¬ê°€ ì¬ê²€í† ë©ë‹ˆë‹¤.")
        state["prompt_hash"] = current_hash
        state["last_prompt_change"] = time.strftime("%Y-%m-%d %H:%M:%S")
        return True
    
    return False

def save_state(state):
    """ìƒíƒœ íŒŒì¼ ì €ì¥"""
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def log_error(error_type, entry, error_message):
    """ì—ëŸ¬ ë¡œê·¸ ì €ì¥"""
    error_log_file = "errors.json"
    errors = []
    
    # ê¸°ì¡´ ì—ëŸ¬ ë¡œê·¸ ë¡œë“œ
    if os.path.exists(error_log_file):
        try:
            with open(error_log_file, "r", encoding="utf-8") as f:
                errors = json.load(f)
        except:
            errors = []
    
    # ìƒˆ ì—ëŸ¬ ì¶”ê°€
    error_entry = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "type": error_type,
        "title": entry.get("title", ""),
        "link": entry.get("link", ""),
        "message": error_message
    }
    errors.append(error_entry)
    
    # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
    errors = errors[-1000:]
    
    # ì €ì¥
    with open(error_log_file, "w", encoding="utf-8") as f:
        json.dump(errors, f, ensure_ascii=False, indent=2)

# -----------------------------
# RSS ìœ í‹¸
# -----------------------------
def entry_uid(e):
    """ê¸°ì‚¬ ê³ ìœ  ID ìƒì„± (link ìš°ì„ )"""
    link = getattr(e, "link", None) or e.get("link")
    if link:
        return link
    return (
        getattr(e, "id", None)
        or getattr(e, "guid", None)
        or e.get("title")
    )

def entry_ts(e):
    """ê¸°ì‚¬ ë°œí–‰ ì‹œê°„ (íƒ€ì„ìŠ¤íƒ¬í”„)"""
    t = getattr(e, "published_parsed", None) or getattr(e, "updated_parsed", None)
    if t:
        return int(time.mktime(t))
    return 0

def fetch_all_recent_entries(max_entries=20):
    """ëª¨ë“  RSS í”¼ë“œì—ì„œ ìµœê·¼ ê¸°ì‚¬ë“¤ì„ ê°€ì ¸ì˜´
    
    RSS í”¼ë“œëŠ” ê³µê°œì ìœ¼ë¡œ ì œê³µë˜ëŠ” í”¼ë“œì´ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    all_entries = []
    
    for url in RSS_URLS:
        try:
            # RSS í”¼ë“œ ìš”ì²­ ê°„ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(0.5)
            r = requests.get(url, headers=HEADERS, timeout=10)
            r.raise_for_status()
            r.encoding = r.apparent_encoding or 'utf-8'
            feed = feedparser.parse(r.text)
            
            if not feed.entries:
                print(f"[DEBUG] RSS í”¼ë“œì— ê¸°ì‚¬ ì—†ìŒ: {url}")
                continue
            
            print(f"[DEBUG] {url}ì—ì„œ {len(feed.entries)}ê±´ ë°œê²¬")
            all_entries.extend(feed.entries)
            
        except Exception as e:
            print(f"[ERROR] fetch failed: {url}, {e}")
            continue
    
    # ì‹œê°„ìˆœ ì •ë ¬
    all_entries.sort(key=entry_ts, reverse=True)
    return all_entries[:max_entries]

def normalize_title(title):
    """ì œëª©ì„ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì²´í¬ì— ì‚¬ìš©"""
    if not title:
        return ""
    normalized = re.sub(r'\s+', '', title.lower())
    normalized = re.sub(r'[^\wê°€-í£]', '', normalized)
    return normalized

def normalize_url(url):
    """canonical_url ìƒì„±: utm, fragment, ëª¨ë°”ì¼ ë„ë©”ì¸ ì œê±° ë“±"""
    if not url:
        return ""
    
    try:
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
        parsed = urlparse(url.strip())
        
        # ë„ë©”ì¸ ì •ê·œí™” (ëª¨ë°”ì¼ ë„ë©”ì¸ â†’ ë°ìŠ¤í¬í†± ë„ë©”ì¸)
        domain = parsed.netloc.lower()
        mobile_to_desktop = {
            'm.': '',
            'mobile.': '',
            'www.m.': 'www.',
        }
        for mobile_prefix, desktop_prefix in mobile_to_desktop.items():
            if domain.startswith(mobile_prefix):
                domain = domain.replace(mobile_prefix, desktop_prefix, 1)
                break
        
        # ê²½ë¡œ ì •ê·œí™” (trailing slash ì œê±°, ë‹¨ ë£¨íŠ¸ëŠ” ìœ ì§€)
        path = parsed.path.rstrip('/') if parsed.path != '/' else '/'
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë¦¬ (utm íŒŒë¼ë¯¸í„° ì œê±°, ë‚˜ë¨¸ì§€ ì •ë ¬)
        query_params = parse_qs(parsed.query)
        # utm ê´€ë ¨ íŒŒë¼ë¯¸í„° ì œê±°
        utm_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content', 'fbclid', 'gclid']
        for utm_param in utm_params:
            query_params.pop(utm_param, None)
        
        # ì •ë ¬ëœ ì¿¼ë¦¬ ìƒì„±
        sorted_query = urlencode(sorted(query_params.items()), doseq=True) if query_params else ''
        
        # fragment ì œê±°
        normalized = urlunparse(parsed._replace(
            netloc=domain,
            path=path,
            query=sorted_query,
            fragment=''
        ))
        
        return normalized.lower()
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ê·œí™”
        url = url.strip().lower()
        if url.endswith('/') and len(url) > 1:
            url = url.rstrip('/')
        return url

def extract_keywords(title):
    """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€, ì˜ë¬¸, ìˆ«ì)"""
    if not title:
        return set()
    
    # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
    korean_words = re.findall(r'[ê°€-í£]+', title)
    # ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
    english_words = re.findall(r'[a-zA-Z]{2,}', title)
    # ìˆ«ì ì¶”ì¶œ
    numbers = re.findall(r'\d+', title)
    
    keywords = set(korean_words + [w.lower() for w in english_words] + numbers)
    return keywords

def is_similar_title(title1, title2, threshold=0.4):
    """ì œëª© ìœ ì‚¬ë„ ì²´í¬ (í‚¤ì›Œë“œ ê¸°ë°˜ Jaccard ìœ ì‚¬ë„)"""
    if not title1 or not title2:
        return False
    
    if title1 == title2:
        return True
    
    keywords1 = extract_keywords(title1)
    keywords2 = extract_keywords(title2)
    
    if not keywords1 or not keywords2:
        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì •ê·œí™”ëœ ë¬¸ìì—´ ê¸¸ì´ ë¹„êµ
        norm1 = normalize_title(title1)
        norm2 = normalize_title(title2)
        if not norm1 or not norm2:
            return False
        # ê¸¸ì´ ì°¨ì´ê°€ 30% ì´ë‚´ë©´ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨
        len_diff = abs(len(norm1) - len(norm2)) / max(len(norm1), len(norm2))
        return len_diff < 0.3
    
    # ì œí’ˆëª…/ê¸°ì—…ëª…ì´ ë‹¤ë¥´ë©´ ìœ ì‚¬í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨
    # "ì œí’ˆ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ê¶Œê³ " í˜•ì‹ì˜ ê¸°ì‚¬ëŠ” ì œí’ˆëª…ìœ¼ë¡œ êµ¬ë¶„
    # ì•Œë ¤ì§„ ì œí’ˆëª…/ê¸°ì—…ëª… íŒ¨í„´ ì¶”ì¶œ (ë” í¬ê´„ì ìœ¼ë¡œ)
    product_patterns = [
        # ì˜ë¬¸ ì œí’ˆëª… (í•˜ì´í”ˆ, ê³µë°± í¬í•¨)
        r'\b([a-z]+[- ]?[a-z]+|[a-z]{3,})\b',
        # í•œê¸€ ê¸°ì—…ëª…/ì œí’ˆëª…
        r'\b([ê°€-í£]{2,})\b'
    ]
    
    # ì œëª©ì—ì„œ ì œí’ˆëª…/ê¸°ì—…ëª… ì¶”ì¶œ (ì¼ë°˜ ë‹¨ì–´ ì œì™¸)
    common_words = {'ì œí’ˆ', 'ë³´ì•ˆ', 'ì—…ë°ì´íŠ¸', 'ê¶Œê³ ', 'ì·¨ì•½ì ', 'íŒ¨ì¹˜', 'ë°œê²¬', 'ìˆ˜ì •', 'ì—…ë°ì´íŠ¸', 'ê¶Œê³ ', 'ë°œí‘œ', 'ê³µê°œ'}
    
    def extract_product_names(title):
        """ì œëª©ì—ì„œ ì œí’ˆëª…/ê¸°ì—…ëª… ì¶”ì¶œ"""
        title_lower = title.lower()
        products = set()
        
        # ì•Œë ¤ì§„ ì œí’ˆëª… íŒ¨í„´ (í•˜ì´í”ˆ, ê³µë°± ì²˜ë¦¬)
        known_products = [
            'tp-link', 'tplink', 'airoha', 'adobe', 'fortigate', 'windows', 'office', 
            'microsoft', 'cisco', 'vmware', 'trend micro', 'hpe', 'mongodb', 'n8n',
            'telegram', 'facebook', 'instagram', 'linkedin', 'gemini', 'google', 'slack', 'zoom'
        ]
        
        # ì•Œë ¤ì§„ ì œí’ˆëª… ë§¤ì¹­ (í•˜ì´í”ˆ, ê³µë°± ë¬´ì‹œ)
        for product in known_products:
            # í•˜ì´í”ˆê³¼ ê³µë°±ì„ ì„ íƒì  ë¬¸ìë¡œ ë³€í™˜
            # ë¬¸ì í´ë˜ìŠ¤ì—ì„œ í•˜ì´í”ˆì€ ë§¨ ë’¤ì— ë°°ì¹˜í•˜ì—¬ ë¦¬í„°ëŸ´ë¡œ ì²˜ë¦¬
            # [-\s] ëŒ€ì‹  [\s-] ì‚¬ìš© (í•˜ì´í”ˆì„ ë§¨ ë’¤ì—)
            escaped_product = re.escape(product)
            # ê³µë°±ê³¼ í•˜ì´í”ˆì„ ì„ íƒì  ë¬¸ìë¡œ ë³€í™˜ (í•˜ì´í”ˆì„ ë§¨ ë’¤ì—)
            pattern = escaped_product.replace(r'\ ', r'[\s-]?').replace(r'\-', r'[\s-]?')
            if re.search(pattern, title_lower, re.IGNORECASE):
                normalized = product.replace(' ', '').replace('-', '').lower()
                products.add(normalized)
        
        # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ ì¶”ì¶œ (ì œí’ˆëª…ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
        # ì˜ˆ: "TP-Link", "Airoha", "Adobe" ë“±
        capitalized_words = re.findall(r'\b([A-Z][a-z]+(?:[- ][A-Z][a-z]+)*)\b', title)
        for word in capitalized_words:
            normalized = word.replace(' ', '').replace('-', '').lower()
            if len(normalized) >= 2 and normalized not in common_words:
                products.add(normalized)
        
        # í•œê¸€ ê¸°ì—…ëª…/ì œí’ˆëª… ì¶”ì¶œ
        korean_words = re.findall(r'\b([ê°€-í£]{2,})\b', title)
        for word in korean_words:
            if word not in common_words:
                products.add(word)
        
        return products
    
    products1 = extract_product_names(title1)
    products2 = extract_product_names(title2)
    
    # ì œí’ˆëª…/ê¸°ì—…ëª…ì´ ìˆê³ , ì„œë¡œ ë‹¤ë¥´ë©´ ìœ ì‚¬í•˜ì§€ ì•ŠìŒ
    # íŠ¹íˆ "ì œí’ˆ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ê¶Œê³ " í˜•ì‹ì˜ ê¸°ì‚¬ëŠ” ì œí’ˆëª…ì´ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ê¸°ì‚¬
    if products1 and products2:
        # ê³µí†µ ì œí’ˆëª…ì´ ì—†ìœ¼ë©´ ìœ ì‚¬í•˜ì§€ ì•ŠìŒ
        if not (products1 & products2):
            return False
    
    # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
    intersection = keywords1 & keywords2
    union = keywords1 | keywords2
    
    if not union:
        return False
    
    similarity = len(intersection) / len(union)
    return similarity >= threshold

# -----------------------------
# AI í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
# -----------------------------
def load_ai_prompt(use_full_content=False):
    """AI íŒë‹¨ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì½ì–´ì˜´
    
    Args:
        use_full_content: Trueë©´ 2ì°¨ íŒë‹¨ í”„ë¡¬í”„íŠ¸, Falseë©´ 1ì°¨ íŒë‹¨ í”„ë¡¬í”„íŠ¸
    """
    if use_full_content:
        prompt_file = AI_PROMPT_FILE_SECOND_STAGE
    else:
        prompt_file = AI_PROMPT_FILE_FIRST_STAGE
    
    if os.path.exists(prompt_file):
        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read().strip()
    else:
        print(f"[ERROR] AI í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_file}")
        raise FileNotFoundError(f"AI í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {prompt_file}")

# ì „ì—­ ë³€ìˆ˜ëŠ” ì œê±°í•˜ê³  í•„ìš”í•  ë•Œë§ˆë‹¤ ë¡œë“œí•˜ë„ë¡ ë³€ê²½

# -----------------------------
# ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
# -----------------------------
def fetch_full_article_content(link, title=""):
    """ê¸°ì‚¬ ë§í¬ì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ì—¬ ê°€ì ¸ì˜´
    
    ì£¼ì˜: RSS í”¼ë“œì—ì„œ ì œê³µëœ ê³µê°œ ë§í¬ë¥¼ ì‚¬ìš©í•˜ë©°, 
    ë³¸ë¬¸ í¬ë¡¤ë§ì€ AI íŒë‹¨ì„ ìœ„í•œ ìµœì†Œí•œì˜ ì •ë³´ ìˆ˜ì§‘ ëª©ì ì…ë‹ˆë‹¤.
    """
    if not link:
        return ""
    
    try:
        # User-Agent ì„¤ì • (ë´‡ ì‹ë³„ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •)
        headers = {
            "User-Agent": "MyrealtripSecurityBot/1.0 (Security News Aggregator; +https://github.com/myrealtrip/security-News-sender)"
        }
        
        # ìš”ì²­ ê°„ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(1)
        
        response = requests.get(link, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # ì‚¬ì´íŠ¸ë³„ ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
        article_content = ""
        
        # 1. ë°ì¼ë¦¬ì‹œí (dailysecu.com)
        if 'dailysecu.com' in link:
            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            article_body = soup.find('div', class_='article-body') or \
                          soup.find('div', id='articleBody') or \
                          soup.find('div', class_='article_view') or \
                          soup.find('article')
            if article_body:
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³  ë“± ì œê±°
                for script in article_body(['script', 'style', 'iframe', 'noscript']):
                    script.decompose()
                article_content = article_body.get_text(separator='\n', strip=True)
        
        # 2. ë³´ì•ˆë‰´ìŠ¤ (boannews.com)
        elif 'boannews.com' in link:
            article_body = soup.find('div', id='news_body_area') or \
                          soup.find('div', class_='article_body') or \
                          soup.find('div', id='articleBody')
            if article_body:
                for script in article_body(['script', 'style', 'iframe', 'noscript']):
                    script.decompose()
                article_content = article_body.get_text(separator='\n', strip=True)
        
        # 3. BOHO (boho.or.kr)
        elif 'boho.or.kr' in link:
            article_body = soup.find('div', class_='view_content') or \
                          soup.find('div', id='content') or \
                          soup.find('div', class_='article-content')
            if article_body:
                for script in article_body(['script', 'style', 'iframe', 'noscript']):
                    script.decompose()
                article_content = article_body.get_text(separator='\n', strip=True)
        
        # 4. ì¼ë°˜ì ì¸ ê²½ìš° (article, main, content ë“± íƒœê·¸ ì‹œë„)
        if not article_content:
            # ì¼ë°˜ì ì¸ ë³¸ë¬¸ íƒœê·¸ ì‹œë„
            for tag_name in ['article', 'main', 'div']:
                for attr in ['class', 'id']:
                    selectors = [
                        {'class': 'article'},
                        {'class': 'content'},
                        {'class': 'article-body'},
                        {'class': 'article-content'},
                        {'id': 'article'},
                        {'id': 'content'},
                        {'id': 'article-body'},
                    ]
                    for selector in selectors:
                        article_body = soup.find(tag_name, selector)
                        if article_body:
                            for script in article_body(['script', 'style', 'iframe', 'noscript']):
                                script.decompose()
                            article_content = article_body.get_text(separator='\n', strip=True)
                            if article_content and len(article_content) > 100:
                                break
                    if article_content:
                        break
                if article_content:
                    break
        
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ bodyì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if not article_content or len(article_content) < 100:
            # bodyì—ì„œ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for script in soup(['script', 'style', 'iframe', 'noscript', 'header', 'footer', 'nav', 'aside']):
                script.decompose()
            article_content = soup.get_text(separator='\n', strip=True)
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ì œì™¸
            if len(article_content) < 100 or len(article_content) > 50000:
                article_content = ""
        
        # ë³¸ë¬¸ ì •ë¦¬ (ê³µë°± ì •ë¦¬, ìµœëŒ€ ê¸¸ì´ ì œí•œ)
        if article_content:
            # ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
            article_content = re.sub(r'\n\s*\n\s*\n+', '\n\n', article_content)
            article_content = article_content.strip()
            # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸° (AI í† í° ì œí•œ ê³ ë ¤)
            if len(article_content) > 10000:
                article_content = article_content[:10000] + "\n\n[ë³¸ë¬¸ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë¨]"
        
        return article_content
        
    except Exception as e:
        print(f"[WARN] ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {link}, ì—ëŸ¬: {e}")
        # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
        log_error("crawl_failed", {"link": link, "title": title}, str(e))
        return ""

# -----------------------------
# AI í”„ë¡¬í”„íŠ¸ ìƒì„±
# -----------------------------
def create_ai_prompt(e, task_description=None, use_full_content=False):
    """ê¸°ì‚¬ ì •ë³´ë¥¼ AIê°€ íŒë‹¨í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    title = e.get("title", "(ì œëª© ì—†ìŒ)")
    link = e.get("link", "")
    summary = e.get("summary", e.get("description", ""))
    published = e.get("published", e.get("updated", ""))
    author = e.get("author", "")
    
    tags = []
    if hasattr(e, "tags") and e.tags:
        tags = [tag.get("term", "") for tag in e.tags if tag.get("term")]
    
    # use_full_contentê°€ Trueì¼ ë•Œë§Œ ì „ì²´ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
    article_body = summary
    if use_full_content and link:
        print(f"[DEBUG] ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œë„: {title[:50]}...")
        full_content = fetch_full_article_content(link, title)
        if full_content:
            print(f"[DEBUG] ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì„±ê³µ: {len(full_content)}ì")
            article_body = full_content
        else:
            print(f"[DEBUG] ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨, RSS ìš”ì•½ ì‚¬ìš©")
    
    article_info = f"""ì œëª©: {title}
ë§í¬: {link}
ë°œí–‰ì¼: {published}
ì‘ì„±ì: {author if author else "(ì •ë³´ ì—†ìŒ)"}
íƒœê·¸: {', '.join(tags) if tags else "(íƒœê·¸ ì—†ìŒ)"}

ê¸°ì‚¬ ë‚´ìš©:
{article_body}"""
    
    if not task_description:
        raise ValueError("í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    korean_instruction = f"""
[ì¤‘ìš”] ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    
    if "[User]" in task_description:
        prompt = task_description.replace(
            "[User]\nEvaluate the following article/advisory/CVE according to the above criteria:\n(Title, date, article content or link)",
            f"[User]\nEvaluate the following article/advisory/CVE according to the above criteria:\n\n{article_info}{korean_instruction}"
        )
    else:
        prompt = f"{task_description}\n\n[User]\nEvaluate the following article/advisory/CVE according to the above criteria:\n\n{article_info}{korean_instruction}"
    
    return prompt

# -----------------------------
# AI íŒë‹¨
# -----------------------------
def judge_with_ai(e, custom_prompt=None, use_full_content=False):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ë¥¼ íŒë‹¨"""
    if not USE_AI_JUDGMENT:
        print("[WARN] USE_AI_JUDGMENTê°€ Falseë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return None
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (use_full_contentì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
    try:
        if custom_prompt:
            task_description = custom_prompt
        else:
            task_description = load_ai_prompt(use_full_content=use_full_content)
    except FileNotFoundError as e:
        print(f"[ERROR] {e}")
        return None
    
    if task_description is None:
        print("[ERROR] AI í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # AI_PROVIDER ê°’ í™•ì¸ ë° ì¶œë ¥ (ë§ˆìŠ¤í‚¹ ë°©ì§€ë¥¼ ìœ„í•´ ê¸¸ì´ì™€ ì²« ê¸€ìë§Œ í‘œì‹œ)
    provider_display = f"{AI_PROVIDER[0]}{'*' * (len(AI_PROVIDER) - 2) if len(AI_PROVIDER) > 2 else '*'}{AI_PROVIDER[-1]}" if len(AI_PROVIDER) > 1 else AI_PROVIDER
    print(f"[DEBUG] AI_PROVIDER ê°’: '{provider_display}' (ì‹¤ì œ ê¸¸ì´: {len(AI_PROVIDER)}, ì†Œë¬¸ì ë³€í™˜ í›„: '{AI_PROVIDER}')")
    print(f"[DEBUG] AI_PROVIDERê°€ 'anthropic'ê³¼ ê°™ì€ê°€? {AI_PROVIDER == 'anthropic'}")
    
    # API í‚¤ í™•ì¸
    if AI_PROVIDER == "anthropic":
        if not ANTHROPIC_API_KEY:
            print("[WARN] ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print(f"[DEBUG] AI_PROVIDER: {AI_PROVIDER}, ANTHROPIC_API_KEY ê¸¸ì´: {len(ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else 0}")
            return None
        print(f"[DEBUG] AI Provider: Anthropic, Model: {ANTHROPIC_MODEL}")
    else:
        if not OPENAI_API_KEY:
            print(f"[WARN] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (í˜„ì¬ AI_PROVIDER: '{AI_PROVIDER}')")
            print(f"[DEBUG] AI_PROVIDER: {AI_PROVIDER}, OPENAI_API_KEY ê¸¸ì´: {len(OPENAI_API_KEY) if OPENAI_API_KEY else 0}")
            print(f"[WARN] AI_PROVIDERê°€ 'anthropic'ì´ ì•„ë‹™ë‹ˆë‹¤. GitHub Secretsì—ì„œ AI_PROVIDERë¥¼ 'anthropic'ìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜, Secretsë¥¼ ì œê±°í•˜ì—¬ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return None
        print(f"[DEBUG] AI Provider: OpenAI, Model: {OPENAI_MODEL}")
    
    try:
        # task_descriptionì€ ì´ë¯¸ ìœ„ì—ì„œ ë¡œë“œë¨
        prompt = create_ai_prompt(e, task_description, use_full_content=use_full_content)
        
        print(f"[DEBUG] í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)} ë¬¸ì)")
        
        # Anthropic (Claude) API í˜¸ì¶œ
        if AI_PROVIDER == "anthropic":
            headers = {
                "x-api-key": ANTHROPIC_API_KEY,
                "anthropic-version": "2023-06-01",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": ANTHROPIC_MODEL,
                "max_tokens": 4096,
                "system": "ë‹¹ì‹ ì€ ë³´ì•ˆ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”. ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }
            
            print(f"[DEBUG] Anthropic API í˜¸ì¶œ ì¤‘...")
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            ai_response = result["content"][0]["text"]
            print(f"[DEBUG] API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(ai_response)} ë¬¸ì)")
        
        # OpenAI API í˜¸ì¶œ
        else:
            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": OPENAI_MODEL,
                "messages": [
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë³´ì•ˆ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”. ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 1000
            }
            
            print(f"[DEBUG] OpenAI API í˜¸ì¶œ ì¤‘...")
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            ai_response = result["choices"][0]["message"]["content"]
            print(f"[DEBUG] API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(ai_response)} ë¬¸ì)")
        
        # JSON íŒŒì‹±
        try:
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            cleaned_response = re.sub(r'```json\s*', '', ai_response)
            cleaned_response = re.sub(r'```\s*', '', cleaned_response)
            cleaned_response = cleaned_response.strip()
            
            # JSON ê°ì²´ ì°¾ê¸°
            brace_count = 0
            start_idx = -1
            json_str = None
            for i, char in enumerate(cleaned_response):
                if char == '{':
                    if start_idx == -1:
                        start_idx = i
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0 and start_idx != -1:
                        json_str = cleaned_response[start_idx:i+1]
                        break
            
            if json_str:
                # JSON ë¬¸ìì—´ ë‚´ë¶€ì˜ ì¤„ë°”ê¿ˆ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                def fix_newlines_in_json_strings(text):
                    """JSON ë¬¸ìì—´ ë‚´ë¶€ì˜ ì¤„ë°”ê¿ˆì„ ì´ìŠ¤ì¼€ì´í”„"""
                    result = []
                    i = 0
                    in_string = False
                    escape_next = False
                    
                    while i < len(text):
                        char = text[i]
                        
                        if escape_next:
                            result.append(char)
                            escape_next = False
                        elif char == '\\':
                            result.append(char)
                            escape_next = True
                        elif char == '"' and not escape_next:
                            in_string = not in_string
                            result.append(char)
                        elif in_string and char == '\n':
                            result.append('\\n')
                        elif in_string and char == '\r':
                            result.append('\\r')
                        else:
                            result.append(char)
                        
                        i += 1
                    
                    return ''.join(result)
                
                json_str = fix_newlines_in_json_strings(json_str)
                judgment = json.loads(json_str)
                judgment["_raw_response"] = ai_response
            else:
                print(f"[WARN] JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì‘ë‹µ:\n{ai_response[:500]}")
                judgment = {"raw_response": ai_response}
        except (json.JSONDecodeError, ValueError) as e:
            print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"[WARN] ì›ë³¸ ì‘ë‹µ:\n{ai_response[:500]}")
            judgment = {"raw_response": ai_response}
        
        return judgment
        
    except Exception as e:
        print(f"[ERROR] AI judgment failed: {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
                print(f"[ERROR] API Error Detail: {error_detail}")
            except:
                try:
                    print(f"[ERROR] Response Status: {e.response.status_code}")
                    print(f"[ERROR] Response Text: {e.response.text[:500]}")
                except:
                    pass
        return None

# -----------------------------
# AI ì¤‘ì‹¬ ë°œì†¡ ê²°ì •
# -----------------------------
def should_send_article_ai_driven(ai_judgment, entry=None):
    """
    AI ì¤‘ì‹¬ ë°œì†¡ ê²°ì • í•¨ìˆ˜
    - AIì˜ decisionì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‹ ë¢°
    - ìµœì†Œí•œì˜ ê²€ì¦ë§Œ ìˆ˜í–‰
    """
    if not ai_judgment:
        print("[INFO] AI íŒë‹¨ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ì ìœ¼ë¡œ ë°œì†¡í•˜ì§€ ì•ŠìŒ")
        return False
    
    # decision í•„ë“œ í™•ì¸
    if "decision" not in ai_judgment:
        print("[WARN] decision í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤. JSON íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ íŒŒì¼ í˜•ì‹ì´ ì•„ë‹Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        return False
    
    decision = ai_judgment.get("decision", "SKIP")
    score = ai_judgment.get("score", 0)
    
    # AI íŒë‹¨ ê·¼ê±° ìƒì„¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    why_list = ai_judgment.get("why", [])
    why_text = " | ".join(why_list) if isinstance(why_list, list) else str(why_list)
    products_affected = ai_judgment.get("products_affected", [])
    products_text = ", ".join(products_affected) if isinstance(products_affected, list) else str(products_affected)
    tags = ai_judgment.get("tags", [])
    tags_text = ", ".join(tags) if isinstance(tags, list) else str(tags)
    
    print(f"[AI íŒë‹¨ ìƒì„¸]")
    print(f"  - Decision: {decision}")
    print(f"  - Score: {score}/100")
    if why_text:
        print(f"  - Why: {why_text}")
    if products_text:
        print(f"  - Products: {products_text}")
    if tags_text:
        print(f"  - Tags: {tags_text}")
    
    # AIì˜ decisionì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ë”°ë¦„
    if decision == "SCRAPE":
        print(f"[INFO] AI íŒë‹¨: SCRAPE (ì ìˆ˜: {score}) â†’ ë°œì†¡")
        return True
    elif decision == "WATCHLIST":
        print(f"[INFO] AI íŒë‹¨: WATCHLIST (ì ìˆ˜: {score}) â†’ ë°œì†¡")
        return True
    elif decision == "SKIP":
        print(f"[INFO] AI íŒë‹¨: SKIP (ì ìˆ˜: {score}) â†’ í•„í„°ë§")
        return False
    else:
        print(f"[WARN] ì•Œ ìˆ˜ ì—†ëŠ” decision: {decision} â†’ í•„í„°ë§")
        return False

# -----------------------------
# Slack ë°œì†¡
# -----------------------------
def post_one_to_slack(e, ai_judgment=None):
    """ìŠ¬ë™ì— ê¸°ì‚¬ ë°œì†¡ (Bot Token ì‚¬ìš©, ë©”ì¸ ë©”ì‹œì§€ + ì“°ë ˆë“œ ìƒì„¸ ì •ë³´)"""
    if not SLACK_BOT_TOKEN or not SLACK_CHANNEL:
        print("âš ï¸  SLACK_BOT_TOKEN ë˜ëŠ” SLACK_CHANNELì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ìŠ¬ë™ ë°œì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print(f"   ê¸°ì‚¬: {e.get('title', '')[:50]}...")
        return
    
    title = e.get("title", "(no title)")
    link = e.get("link", "")
    published = e.get("published", "")
    
    # ë‚ ì§œ í¬ë§·íŒ…
    from datetime import datetime
    date_str = ""
    if published:
        try:
            # RSS ë‚ ì§œ íŒŒì‹± ì‹œë„
            if isinstance(published, str):
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S", "%a, %d %b %Y %H:%M:%S %z"]:
                    try:
                        dt = datetime.strptime(published, fmt)
                        date_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                        break
                    except:
                        continue
            if not date_str:
                date_str = str(published)[:19]  # ì²˜ìŒ 19ìë§Œ (YYYY-MM-DD HH:MM:SS)
        except:
            date_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    else:
        date_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ìœ„í—˜ë„ ê²°ì • (score ê¸°ë°˜)
    risk_level = ""
    risk_emoji = ""
    score = 0
    if ai_judgment and "score" in ai_judgment:
        score = ai_judgment.get("score", 0)
        if score >= 81:
            risk_level = "ë†’ìŒ"
            risk_emoji = "ğŸ”´"
        elif score >= 51:
            risk_level = "ì¤‘ê°„"
            risk_emoji = "ğŸŸ¡"
        else:
            risk_level = "ë‚®ìŒ"
            risk_emoji = "ğŸŸ¢"
    elif ai_judgment and "severity" in ai_judgment:
        severity = ai_judgment.get("severity", "Unknown")
        if severity in ["Critical", "High"]:
            risk_level = "ë†’ìŒ"
            risk_emoji = "ğŸ”´"
        elif severity == "Medium":
            risk_level = "ì¤‘ê°„"
            risk_emoji = "ğŸŸ¡"
        else:
            risk_level = "ë‚®ìŒ"
            risk_emoji = "ğŸŸ¢"
    
    # ì œëª© ìƒì„±: AI ìš”ì•½ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ì›ë³¸ ì œëª©ì„ ì‹¬í”Œí•˜ê²Œ ì •ì œ
    import re
    clean_title = title
    
    # AI ìš”ì•½ì—ì„œ ì œëª© ìƒì„± ì‹œë„ (ëŒ€ìƒ + í•µì‹¬ ë‚´ìš©)
    if ai_judgment and ai_judgment.get("summary_3lines"):
        summary_text = ai_judgment['summary_3lines']
        summary_lines = summary_text.split('\n')
        
        target_for_title = ""
        content_for_title = ""
        
        for line in summary_lines:
            line = line.strip()
            if not line:
                continue
            if '|' in line and (line.startswith('ğŸ”´') or line.startswith('ğŸŸ¡') or line.startswith('ğŸŸ¢')):
                continue
            clean_line = line.replace('ğŸ”´', '').replace('ğŸŸ¡', '').replace('ğŸŸ¢', '').replace('ğŸ¯', '').replace('ğŸ“Š', '').replace('ğŸ“…', '').strip()
            
            if clean_line.startswith('ëŒ€ìƒ:') or clean_line.startswith('ëŒ€ìƒ ì‹œìŠ¤í…œ:'):
                target_text = clean_line.split(':', 1)[1].strip() if ':' in clean_line else clean_line
                # ì œí’ˆëª…ë§Œ ì¶”ì¶œ (ë²„ì „ ì •ë³´ ì œê±°)
                version_pattern = r'\d+\.\d+[\.\d]*[^\s]*'
                target_for_title = re.sub(version_pattern, '', target_text).strip()
                target_for_title = re.sub(r'\s*(ë¯¸ë§Œ|ì´í•˜|ì´ìƒ|ë²„ì „)', '', target_for_title).strip()
                # ê´„í˜¸ ë‚´ìš© ì œê±°
                target_for_title = re.sub(r'\([^)]*\)', '', target_for_title).strip()
            elif clean_line.startswith('ë‚´ìš©:') or clean_line.startswith('ì„¤ëª…:'):
                content_text = clean_line.split(':', 1)[1].strip() if ':' in clean_line else clean_line
                # AI ìš”ì•½ì˜ ì „ì²´ ë‚´ìš© ì‚¬ìš© (ìë¥´ì§€ ì•ŠìŒ)
                content_for_title = content_text.strip()
        
        # ì œëª© ìƒì„±: [ì œí’ˆëª…] [í•µì‹¬ ë‚´ìš©]
        if target_for_title and content_for_title:
            clean_title = f"{target_for_title} {content_for_title}"
        elif target_for_title:
            # ì œí’ˆëª…ë§Œ ìˆëŠ” ê²½ìš°
            clean_title = f"{target_for_title} ë³´ì•ˆ ì´ìŠˆ"
        elif content_for_title:
            # ë‚´ìš©ë§Œ ìˆëŠ” ê²½ìš°
            clean_title = content_for_title
    
    # ì›ë³¸ ì œëª© ì •ì œ (AI ìš”ì•½ì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ)
    if clean_title == title:
        # ê°ì •ì  í‘œí˜„ ë° ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        emotional_words = ["ë¹„ìƒ", "ì¶©ê²©", "ê¸´ê¸‰", "ê²½ê³ ", "ì£¼ì˜", "ë°œê²¬", "í™•ì¸", "ì£¼ì˜ë³´", "ê²½ê³ ", "ë¹„ìƒì‚¬íƒœ"]
        for word in emotional_words:
            clean_title = clean_title.replace(word, "")
        
        # ë¶ˆí•„ìš”í•œ êµ¬ë‘ì  ë° í‘œí˜„ ì œê±°
        clean_title = re.sub(r'\.\.\.+', '', clean_title)  # ... ì œê±°
        clean_title = re.sub(r'["""]', '', clean_title)  # ë”°ì˜´í‘œ ì œê±°
        clean_title = re.sub(r'\s+', ' ', clean_title).strip()
    
    # ìµœì¢… ì •ì œ (ìë¥´ì§€ ì•ŠìŒ)
    clean_title = re.sub(r'\s+', ' ', clean_title).strip()
    
    # ëŒ€ìƒ ì •ë³´ ì¶”ì¶œ (ìƒì„¸í•˜ê²Œ)
    target_product = ""
    vulnerable_version = ""
    if ai_judgment and ai_judgment.get("summary_3lines"):
        summary_text = ai_judgment['summary_3lines']
        summary_lines = summary_text.split('\n')
        for line in summary_lines:
            line = line.strip()
            if not line:
                continue
            if '|' in line and (line.startswith('ğŸ”´') or line.startswith('ğŸŸ¡') or line.startswith('ğŸŸ¢')):
                continue
            clean_line = line.replace('ğŸ”´', '').replace('ğŸŸ¡', '').replace('ğŸŸ¢', '').replace('ğŸ¯', '').replace('ğŸ“Š', '').replace('ğŸ“…', '').strip()
            if clean_line and (clean_line.startswith('ëŒ€ìƒ:') or clean_line.startswith('ëŒ€ìƒ ì‹œìŠ¤í…œ:')):
                target_text = clean_line.split(':', 1)[1].strip() if ':' in clean_line else clean_line
                target_product = target_text  # ë²„ì „ ì •ë³´ í¬í•¨í•˜ì—¬ ì „ì²´ ì‚¬ìš©
                # ë²„ì „ ì •ë³´ë„ ë³„ë„ë¡œ ì¶”ì¶œ
                version_pattern = r'\d+\.\d+[\.\d]*[^\s]*'
                version_match = re.search(version_pattern, target_text)
                if version_match:
                    if any(keyword in target_text for keyword in ['ë¯¸ë§Œ', 'ì´í•˜', 'ì´ìƒ']):
                        vulnerable_version = target_text
                    else:
                        vulnerable_version = version_match.group(0)
                break
    
    # ì œí’ˆëª…ì´ ì—†ìœ¼ë©´ products_affectedì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if not target_product and ai_judgment:
        products = ai_judgment.get("products_affected", [])
        if isinstance(products, list) and products:
            target_product = products[0]
        elif isinstance(products, str) and products:
            target_product = products
    
    # ë‚´ìš© ì¶”ì¶œ (ìƒì„¸í•˜ê²Œ)
    content_detail = ""
    if ai_judgment and ai_judgment.get("summary_3lines"):
        summary_text = ai_judgment['summary_3lines']
        summary_lines = summary_text.split('\n')
        for line in summary_lines:
            line = line.strip()
            if not line:
                continue
            # ìœ„í—˜ë„ ë¼ì¸ì€ ì œì™¸
            if '|' in line and (line.startswith('ğŸ”´') or line.startswith('ğŸŸ¡') or line.startswith('ğŸŸ¢')):
                continue
            # ì´ëª¨ì§€ ì œê±°
            clean_line = line.replace('ğŸ”´', '').replace('ğŸŸ¡', '').replace('ğŸŸ¢', '').replace('ğŸ¯', '').replace('ğŸ“Š', '').replace('ğŸ“…', '').strip()
            if clean_line:
                # "ë‚´ìš©:", "ì„¤ëª…:" ë¼ì¸ ì°¾ê¸°
                if clean_line.startswith('ë‚´ìš©:') or clean_line.startswith('ì„¤ëª…:'):
                    content_detail = clean_line.split(':', 1)[1].strip() if ':' in clean_line else clean_line
                    break
                # "ëŒ€ìƒ:"ì´ ì•„ë‹Œ ë‹¤ë¥¸ ë¼ì¸ë„ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš© (ëŒ€ìƒ ë¼ì¸ ë‹¤ìŒ)
                elif not clean_line.startswith('ëŒ€ìƒ:') and not clean_line.startswith('ëŒ€ìƒ ì‹œìŠ¤í…œ:'):
                    # ì´ë¯¸ ëŒ€ìƒì´ ì¶”ì¶œë˜ì—ˆê³ , ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì´ ë¼ì¸ì„ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©
                    if target_product and not content_detail:
                        content_detail = clean_line.split(':', 1)[1].strip() if ':' in clean_line else clean_line
                        break
    
    # ë‚´ìš©ì´ ì—†ìœ¼ë©´ whyì—ì„œ ì¶”ì¶œ
    if not content_detail and ai_judgment:
        why_list = ai_judgment.get("why", [])
        if isinstance(why_list, list) and why_list:
            content_detail = why_list[0]  # ì²« ë²ˆì§¸ ì´ìœ ë¥¼ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©
        elif isinstance(why_list, str) and why_list:
            content_detail = why_list
    
    # ê¶Œê³ ì‚¬í•­ ì¶”ì¶œ
    recommended_action = ""
    if ai_judgment:
        actions = ai_judgment.get("recommended_actions", [])
        if isinstance(actions, list) and actions:
            recommended_action = actions[0]  # ì²« ë²ˆì§¸ ê¶Œê³ ì‚¬í•­ë§Œ
        elif isinstance(actions, str) and actions:
            recommended_action = actions
        
        # summaryì—ì„œë„ ê¶Œì¥ ì¡°ì¹˜ ì¶”ì¶œ ì‹œë„
        if not recommended_action and ai_judgment.get("summary_3lines"):
            summary_text = ai_judgment['summary_3lines']
            # "ì—…ë°ì´íŠ¸", "íŒ¨ì¹˜", "ì—…ê·¸ë ˆì´ë“œ" ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ì¶œ
            if re.search(r'(ì—…ë°ì´íŠ¸|íŒ¨ì¹˜|ì—…ê·¸ë ˆì´ë“œ|ì ê²€|ê¶Œì¥)', summary_text, re.IGNORECASE):
                # ê°„ë‹¨í•œ ê¶Œì¥ ì¡°ì¹˜ ìƒì„±
                if vulnerable_version:
                    recommended_action = f"{target_product} ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸"
                else:
                    recommended_action = "ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê¶Œì¥"
    
    # ìœ í˜• ì¶”ì¶œ (tags ê¸°ë°˜)
    article_type = ""
    if ai_judgment:
        tags = ai_judgment.get("tags", [])
        if isinstance(tags, str):
            tags = [tags] if tags else []
        
        # tagsë¥¼ í•œêµ­ì–´ ìœ í˜•ìœ¼ë¡œ ë³€í™˜
        type_mapping = {
            "vulnerability": "ì·¨ì•½ì ",
            "cve": "ì·¨ì•½ì ",
            "phishing": "í”¼ì‹±",
            "social engineering": "ì‚¬íšŒê³µí•™",
            "cyber attack": "ê³µê²©",
            "attack": "ê³µê²©",
            "ransomware": "ëœì„¬ì›¨ì–´",
            "malware": "ì•…ì„±ì½”ë“œ",
            "data breach": "ë°ì´í„° ìœ ì¶œ",
            "data leak": "ë°ì´í„° ìœ ì¶œ",
            "insider threat": "ë‚´ë¶€ì ìœ„í˜‘",
            "sso": "SSO ê³µê²©",
            "mfa bypass": "MFA ìš°íšŒ",
            "web security": "ì›¹ ë³´ì•ˆ",
            "network service": "ë„¤íŠ¸ì›Œí¬ ì„œë¹„ìŠ¤",
            "open source": "ì˜¤í”ˆì†ŒìŠ¤",
            "north korean hackers": "ë¶í•œ í•´ì»¤",
            "north korean": "ë¶í•œ í•´ì»¤"
        }
        
        # tagsì—ì„œ ìœ í˜• ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: ì·¨ì•½ì  > í”¼ì‹±/ì‚¬íšŒê³µí•™ > ê³µê²© > ê¸°íƒ€)
        found_types = []
        tags_lower = [str(tag).lower() for tag in tags]
        
        # ì·¨ì•½ì  ê´€ë ¨
        if any(keyword in " ".join(tags_lower) for keyword in ["vulnerability", "cve"]):
            found_types.append("ì·¨ì•½ì ")
        
        # í”¼ì‹±/ì‚¬íšŒê³µí•™ ê´€ë ¨
        if any(keyword in " ".join(tags_lower) for keyword in ["phishing", "social engineering"]):
            found_types.append("í”¼ì‹±/ì‚¬íšŒê³µí•™")
        
        # ê³µê²© ê´€ë ¨
        if any(keyword in " ".join(tags_lower) for keyword in ["cyber attack", "attack", "ransomware", "malware"]):
            if "ì·¨ì•½ì " not in found_types:  # ì·¨ì•½ì ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê³µê²©ì€ ë³„ë„ë¡œ í‘œì‹œí•˜ì§€ ì•ŠìŒ
                found_types.append("ê³µê²©")
        
        # ë°ì´í„° ìœ ì¶œ ê´€ë ¨
        if any(keyword in " ".join(tags_lower) for keyword in ["data breach", "data leak"]):
            found_types.append("ë°ì´í„° ìœ ì¶œ")
        
        # ë‚´ë¶€ì ìœ„í˜‘
        if "insider threat" in " ".join(tags_lower):
            found_types.append("ë‚´ë¶€ì ìœ„í˜‘")
        
        # ê¸°íƒ€ ë§¤í•‘ë˜ì§€ ì•Šì€ íƒœê·¸ë„ í™•ì¸
        for tag in tags_lower:
            for key, value in type_mapping.items():
                if key in tag and value not in found_types:
                    found_types.append(value)
                    break
        
        if found_types:
            article_type = " â€¢ ".join(found_types[:2])  # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í‘œì‹œ
    
    # ë©”ì¸ ë©”ì‹œì§€ ë¸”ë¡ (ìƒˆë¡œìš´ í˜•ì‹)
    main_blocks = []
    
    # í—¤ë”: ê¸°ì‚¬ ì œëª© (AI ìš”ì•½ ê¸°ë°˜, ìë¥´ì§€ ì•ŠìŒ)
    header_title = f"ğŸ” {clean_title}"
    
    main_blocks.append({
        "type": "header",
        "text": {
            "type": "plain_text",
            "text": header_title,
            "emoji": True
        }
    })
    
    # Context: ë‚ ì§œ, ìœ„í—˜ë„, ìœ í˜•
    context_parts = [f"ë‚ ì§œ: {date_str}", f"ìœ„í—˜ë„: *{score}/100*"]
    if article_type:
        context_parts.append(f"ìœ í˜•: *{article_type}*")
    
    main_blocks.append({
        "type": "context",
        "elements": [
            {
                "type": "mrkdwn",
                "text": " â€¢ ".join(context_parts)
            }
        ]
    })
    
    # ëŒ€ìƒ, ë‚´ìš©, ê¶Œê³ ì‚¬í•­ì„ í•˜ë‚˜ì˜ ë¸”ë¡ì— ì¤„ë°”ê¿ˆìœ¼ë¡œ í‘œì‹œ
    detail_text_parts = []
    if target_product:
        detail_text_parts.append(f"*ëŒ€ìƒ:* {target_product}")
    if content_detail:
        detail_text_parts.append(f"*ë‚´ìš©:* {content_detail}")
    if recommended_action:
        detail_text_parts.append(f"*ê¶Œê³ ì‚¬í•­:* {recommended_action}")
    
    if detail_text_parts:
        main_blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": "\n".join(detail_text_parts)
            }
        })
    
    # ì›ë¬¸ ë§í¬ (ë¯¸ë¦¬ë³´ê¸° ë¹„í™œì„±í™”)
    if link:
        main_blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"ğŸ”— <{link}|ì›ë¬¸ ê¸°ì‚¬ ë³´ëŸ¬ê°€ê¸°>"
            }
        })
    
    # ë©”ì¸ ë©”ì‹œì§€ ì „ì†¡
    headers = {
        "Authorization": f"Bearer {SLACK_BOT_TOKEN}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "channel": SLACK_CHANNEL,
        "blocks": main_blocks,
        "unfurl_links": False,  # ë§í¬ ë¯¸ë¦¬ë³´ê¸° ë¹„í™œì„±í™”
        "unfurl_media": False   # ë¯¸ë””ì–´ ë¯¸ë¦¬ë³´ê¸° ë¹„í™œì„±í™”
    }
    
    resp = requests.post(
        "https://slack.com/api/chat.postMessage",
        headers=headers,
        json=payload,
        timeout=10,
    )
    resp.raise_for_status()
    result = resp.json()
    
    if not result.get("ok"):
        print(f"[ERROR] ìŠ¬ë™ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        return
    
    # ìŠ¤ë ˆë“œ ë©”ì‹œì§€ ì „ì†¡ ì œê±° (ì‚¬ìš©ì ìš”ì²­)

# -----------------------------
# ë©”ì¸ ì²˜ë¦¬ (AI ì¤‘ì‹¬)
# -----------------------------
def process_articles_ai_driven():
    """
    AI ì¤‘ì‹¬ ê¸°ì‚¬ ì²˜ë¦¬
    - ì¤‘ë³µ ì²´í¬ë§Œ ì½”ë“œë¡œ ìˆ˜í–‰
    - AI íŒë‹¨ì„ ìµœëŒ€í•œ ì‹ ë¢°
    """
    state = load_state()
    
    # ë°œì†¡ëœ ê¸°ì‚¬ ì¶”ì  (ì¬ë°œì†¡ ë°©ì§€ìš©)
    sent_links = set(state.get("sent_links", []))
    if "sent_links" not in state:
        state["sent_links"] = []
    
    seen = set(state.get("seen", []))
    seen_titles = set(state.get("seen_titles", []))
    seen_original_titles = state.get("seen_original_titles", [])
    seen_links = set(state.get("seen_links", []))
    
    # ìµœê·¼ ê¸°ì‚¬ë“¤ ê°€ì ¸ì˜¤ê¸°
    recent_entries = fetch_all_recent_entries(max_entries=20)
    
    if not recent_entries:
        print("âŒ No RSS entries found.")
        return 0
    
    print(f"\n[INFO] ì´ {len(recent_entries)}ê±´ì˜ ê¸°ì‚¬ ë°œê²¬")
    
    # 1ë‹¨ê³„: ì¤‘ë³µ ì²´í¬ (ì½”ë“œ ê¸°ë°˜)
    candidate_entries = []
    
    for entry in recent_entries:
        link = entry.get("link", "")
        title = entry.get("title", "")
        
        # ë§í¬ URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        if link:
            normalized_link = normalize_url(link)
            if normalized_link and normalized_link in seen_links:
                print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ ë§í¬ ë°œê²¬: {title[:50]}...")
                continue
        
        uid = entry_uid(entry)
        normalized_title = normalize_title(title)
        
        # UID ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        if uid and uid in seen:
            print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ UID ë°œê²¬: {title[:50]}...")
            continue
        
        # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        if normalized_title and normalized_title in seen_titles:
            print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ ì œëª© ë°œê²¬: {title[:50]}...")
            continue
        
        # ìœ ì‚¬ ì œëª© ì²´í¬
        is_duplicate = False
        for seen_title in seen_original_titles:
            if is_similar_title(title, seen_title):
                print(f"â­ï¸ ìœ ì‚¬í•œ ê¸°ì‚¬ ì œëª© ë°œê²¬: {title[:50]}... (ê¸°ì¡´: {seen_title[:50]}...)")
                is_duplicate = True
                break
        
        if is_duplicate:
            continue
        
        candidate_entries.append(entry)
    
    print(f"[INFO] ì¤‘ë³µ ì œê±° í›„ {len(candidate_entries)}ê±´ì˜ ì‹ ê·œ ê¸°ì‚¬")
    
    if not candidate_entries:
        print("âœ… ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    # 2ë‹¨ê³„: ê°™ì€ ë°°ì¹˜ ë‚´ ìœ ì‚¬ ì œëª© ì¤‘ë³µ ì œê±° (ìµœì‹  ê²ƒë§Œ ì„ íƒ)
    final_entries = []
    for i, entry in enumerate(candidate_entries):
        title = entry.get("title", "")
        is_duplicate_in_batch = False
        
        for j, other_entry in enumerate(candidate_entries):
            if i >= j:
                continue
            other_title = other_entry.get("title", "")
            if title == other_title:
                continue
            if is_similar_title(title, other_title):
                # ë” ìµœì‹  ê¸°ì‚¬ ì„ íƒ
                if entry_ts(entry) < entry_ts(other_entry):
                    is_duplicate_in_batch = True
                    break
        
        # ì¶”ê°€: ê°™ì€ ê¸°ì—…ëª… + ìœ ì‚¬í•œ ë³´ì•ˆ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        if not is_duplicate_in_batch:
            title_lower = title.lower()
            # í•œêµ­ ê¸°ì—…ëª… íŒ¨í„´ ì¶”ì¶œ
            korean_companies = ["êµì›", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "ì‚¼ì„±", "LG", "SK", "í˜„ëŒ€", "ê¸°ì•„", "ë¡¯ë°", "í•œí™”", "ë‘ì‚°", "í¬ìŠ¤ì½”", "KT", "ì‹ í•œ", "ë¼ì¸", "ì¿ íŒ¡"]
            security_keywords = ["í•´í‚¹", "ëœì„¬ì›¨ì–´", "ì •ë³´ìœ ì¶œ", "ì¹¨í•´", "ê³µê²©", "ì‚¬ê³ ", "ìœ ì¶œ"]
            
            for company in korean_companies:
                if company in title_lower:
                    # ê°™ì€ ê¸°ì—…ëª…ì´ ìˆëŠ” ë‹¤ë¥¸ ê¸°ì‚¬ ì°¾ê¸°
                    for j, other_entry in enumerate(candidate_entries):
                        if i >= j:
                            continue
                        other_title = other_entry.get("title", "")
                        other_title_lower = other_title.lower()
                        
                        # ê°™ì€ ê¸°ì—…ëª… + ë³´ì•ˆ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µ ê°€ëŠ¥ì„±
                        if company in other_title_lower:
                            # ë‘˜ ë‹¤ ë³´ì•ˆ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
                            has_security1 = any(keyword in title_lower for keyword in security_keywords)
                            has_security2 = any(keyword in other_title_lower for keyword in security_keywords)
                            
                            if has_security1 and has_security2:
                                # ë” ìµœì‹  ê¸°ì‚¬ ì„ íƒ
                                if entry_ts(entry) < entry_ts(other_entry):
                                    is_duplicate_in_batch = True
                                    print(f"â­ï¸ ê°™ì€ ê¸°ì—…({company})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ê¸°ì‚¬ ì¤‘ë³µ ë°œê²¬: {title[:50]}... (ìµœì‹  ê¸°ì‚¬ ì„ íƒ)")
                                    break
                    if is_duplicate_in_batch:
                        break
            
            # ì œí’ˆëª… + ë³´ì•ˆ ì—…ë°ì´íŠ¸ í‚¤ì›Œë“œ ì¤‘ë³µ ì²´í¬ (AI íŒë‹¨ ì „ì´ë¯€ë¡œ ì œëª© ê¸°ë°˜)
            # ì£¼ì˜: ì´ ë¶€ë¶„ì€ AI íŒë‹¨ ì „ì´ë¯€ë¡œ ì œëª© í‚¤ì›Œë“œ ë§¤ì¹­ ì‚¬ìš©
            # ë°œì†¡ ì „ ì¤‘ë³µ ì²´í¬ì—ì„œëŠ” AIì˜ products_affected í•„ë“œë¥¼ í™œìš©í•¨
            if not is_duplicate_in_batch:
                product_keywords = {
                    "windows": ["windows", "ìœˆë„ìš°", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "ms", "microsoft"],
                    "office": ["office", "ì˜¤í”¼ìŠ¤", "office 365", "microsoft office"],
                    "adobe reader": ["adobe reader", "ì–´ë„ë¹„ ë¦¬ë”", "adobe acrobat reader"],
                    "fortigate": ["fortigate", "í¬í‹°ê²Œì´íŠ¸", "fortios"]
                }
                update_keywords = ["íŒ¨ì¹˜", "ì—…ë°ì´íŠ¸", "ë³´ì•ˆ ì—…ë°ì´íŠ¸", "ì·¨ì•½ì ", "íŒ¨ì¹˜ í™”ìš”ì¼", "ë³´ì•ˆ ìœ„í˜‘", "ë³´ì•ˆ ê¶Œê³ ", "cve"]
                
                for product, product_names in product_keywords.items():
                    if any(name in title_lower for name in product_names):
                        # ê°™ì€ ì œí’ˆì˜ ë‹¤ë¥¸ ê¸°ì‚¬ ì°¾ê¸°
                        for j, other_entry in enumerate(candidate_entries):
                            if i >= j:
                                continue
                            other_title = other_entry.get("title", "")
                            other_title_lower = other_title.lower()
                            
                            # ê°™ì€ ì œí’ˆëª…ì´ ìˆëŠ”ì§€ í™•ì¸
                            if any(name in other_title_lower for name in product_names):
                                # ë‘˜ ë‹¤ ì—…ë°ì´íŠ¸/íŒ¨ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µ ê°€ëŠ¥ì„±
                                has_update1 = any(keyword in title_lower for keyword in update_keywords)
                                has_update2 = any(keyword in other_title_lower for keyword in update_keywords)
                                
                                if has_update1 and has_update2:
                                    # ê°™ì€ ì œí’ˆì˜ ê°™ì€ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ì´ìŠˆë¡œ ê°„ì£¼
                                    # ë” ìµœì‹  ê¸°ì‚¬ ì„ íƒ
                                    if entry_ts(entry) < entry_ts(other_entry):
                                        is_duplicate_in_batch = True
                                        print(f"â­ï¸ ê°™ì€ ì œí’ˆ({product})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ê¸°ì‚¬ ì¤‘ë³µ ë°œê²¬: {title[:50]}... (ìµœì‹  ê¸°ì‚¬ ì„ íƒ)")
                                        break
                        if is_duplicate_in_batch:
                            break
                    if is_duplicate_in_batch:
                        break
        
        if not is_duplicate_in_batch:
            final_entries.append(entry)
    
    print(f"[INFO] ë°°ì¹˜ ë‚´ ì¤‘ë³µ ì œê±° í›„ {len(final_entries)}ê±´ì˜ ê¸°ì‚¬")
    
    # 3ë‹¨ê³„: AI íŒë‹¨ (2ë‹¨ê³„ ë°©ì‹)
    # 3-1: RSS ìš”ì•½ìœ¼ë¡œ 1ì°¨ íŒë‹¨
    print(f"\n[INFO] 1ì°¨ íŒë‹¨: RSS ìš”ì•½ìœ¼ë¡œ AI íŒë‹¨ ì‹œì‘...")
    first_stage_candidates = []  # SCRAPE/WATCHLISTë¡œ íŒë‹¨ëœ ê¸°ì‚¬ë“¤
    new_entries = []
    ai_call_count = 0
    sent_count = 0
    sent_entries = []  # ë°œì†¡ëœ ê¸°ì‚¬ ëª©ë¡ (ì¤‘ë³µ ì²´í¬ìš©) - (entry, ai_judgment) íŠœí”Œ ì €ì¥
    
    for entry in final_entries:
        title = entry.get("title", "")
        link = entry.get("link", "")
        uid = entry_uid(entry)
        normalized_title = normalize_title(title)
        
        print(f"\n[AI íŒë‹¨ ì‹œì‘] {title[:60]}...")
        
        # 1ì°¨ íŒë‹¨: RSS ìš”ì•½ìœ¼ë¡œ íŒë‹¨ (í¬ë¡¤ë§ ì—†ì´)
        ai_judgment = judge_with_ai(entry, use_full_content=False)
        ai_call_count += 1
        
        if not ai_judgment:
            print(f"[WARN] AI íŒë‹¨ ì‹¤íŒ¨: {title[:50]}...")
            continue
        
        # 1ì°¨ íŒë‹¨ ê²°ê³¼ ì €ì¥
        decision = ai_judgment.get("decision", "SKIP")
        if decision in ["SCRAPE", "WATCHLIST"]:
            # ë°œì†¡ í›„ë³´ë¡œ ì¶”ê°€ (ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§ í›„ ì¬ê²€ìˆ˜)
            first_stage_candidates.append((entry, ai_judgment))
            print(f"[INFO] 1ì°¨ íŒë‹¨: {decision} (ì ìˆ˜: {ai_judgment.get('score', 0)}) â†’ ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§ í›„ ì¬ê²€ìˆ˜ ì˜ˆì •")
        else:
            # SKIPìœ¼ë¡œ íŒë‹¨ëœ ê¸°ì‚¬ëŠ” ë°”ë¡œ ì œì™¸
            print(f"[INFO] 1ì°¨ íŒë‹¨: {decision} (ì ìˆ˜: {ai_judgment.get('score', 0)}) â†’ í•„í„°ë§")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (1ì°¨ íŒë‹¨ ê²°ê³¼ë„ ì €ì¥)
        if uid:
            seen.add(uid)
        if normalized_title:
            seen_titles.add(normalized_title)
        if title:
            seen_original_titles.append(title)
        if link:
            normalized_link = normalize_url(link)
            if normalized_link:
                seen_links.add(normalized_link)
        
        new_entries.append({
            "uid": uid or normalized_title or title,
            "title": title,
            "link": link
        })
    
    # 3-2: ë°œì†¡ í›„ë³´ ê¸°ì‚¬ë“¤ë§Œ ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§ í›„ ì¬ê²€ìˆ˜
    if first_stage_candidates:
        print(f"\n[INFO] 2ì°¨ íŒë‹¨: {len(first_stage_candidates)}ê±´ì˜ ë°œì†¡ í›„ë³´ ê¸°ì‚¬ë¥¼ ì „ì²´ ë³¸ë¬¸ìœ¼ë¡œ ì¬ê²€ìˆ˜í•©ë‹ˆë‹¤...")
        
        for entry, first_judgment in first_stage_candidates:
            title = entry.get("title", "")
            link = entry.get("link", "")
            
            print(f"\n[2ì°¨ íŒë‹¨ ì‹œì‘] {title[:60]}...")
            
            # ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§ í›„ ì¬ê²€ìˆ˜
            try:
                ai_judgment = judge_with_ai(entry, use_full_content=True)
                ai_call_count += 1
                
                if not ai_judgment:
                    print(f"[WARN] 2ì°¨ AI íŒë‹¨ ì‹¤íŒ¨: {title[:50]}...")
                    # 1ì°¨ íŒë‹¨ ê²°ê³¼ ì‚¬ìš©
                    ai_judgment = first_judgment
                    # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
                    log_error("ai_judgment_failed", entry, "2ì°¨ AI íŒë‹¨ ì‹¤íŒ¨")
            except Exception as e:
                print(f"[ERROR] 2ì°¨ AI íŒë‹¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {title[:50]}... - {str(e)}")
                # 1ì°¨ íŒë‹¨ ê²°ê³¼ ì‚¬ìš©
                ai_judgment = first_judgment
                # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
                log_error("ai_judgment_error", entry, str(e))
            else:
                print(f"[INFO] 2ì°¨ íŒë‹¨ ì™„ë£Œ: {ai_judgment.get('decision', 'UNKNOWN')} (ì ìˆ˜: {ai_judgment.get('score', 0)})")
            
            # AI ì¤‘ì‹¬ ë°œì†¡ ê²°ì •
            if should_send_article_ai_driven(ai_judgment, entry):
                # ë°œì†¡ ì „ ì¤‘ë³µ ì²´í¬: ê°™ì€ ë°°ì¹˜ ë‚´ì—ì„œ ì´ë¯¸ ë°œì†¡ëœ ìœ ì‚¬í•œ ê¸°ì‚¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                is_duplicate_sent = False
                title_lower = title.lower()
                
                for sent_entry_data in sent_entries:
                    sent_entry, sent_ai_judgment = sent_entry_data
                    sent_title = sent_entry.get("title", "")
                    sent_title_lower = sent_title.lower()
                    
                    # ì •í™•íˆ ê°™ì€ ì œëª©
                    if title == sent_title:
                        is_duplicate_sent = True
                        print(f"â­ï¸ ì´ë¯¸ ë°œì†¡ëœ ë™ì¼ ì œëª© ê¸°ì‚¬ ë°œê²¬: {title[:50]}...")
                        break
                
                    # ìœ ì‚¬í•œ ì œëª©
                    if is_similar_title(title, sent_title, threshold=0.3):
                        is_duplicate_sent = True
                        print(f"â­ï¸ ì´ë¯¸ ë°œì†¡ëœ ìœ ì‚¬ ì œëª© ê¸°ì‚¬ ë°œê²¬: {title[:50]}... (ê¸°ì¡´: {sent_title[:50]}...)")
                        break
                    
                    # ê°™ì€ ê¸°ì—…ëª… + ë³´ì•ˆ í‚¤ì›Œë“œ
                    korean_companies = ["êµì›", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "ì‚¼ì„±", "LG", "SK", "í˜„ëŒ€", "ê¸°ì•„", "ë¡¯ë°", "í•œí™”", "ë‘ì‚°", "í¬ìŠ¤ì½”", "KT", "ì‹ í•œ", "ë¼ì¸", "ì¿ íŒ¡"]
                    security_keywords = ["í•´í‚¹", "ëœì„¬ì›¨ì–´", "ì •ë³´ìœ ì¶œ", "ì¹¨í•´", "ê³µê²©", "ì‚¬ê³ ", "ìœ ì¶œ"]
                    
                    for company in korean_companies:
                        if company in title_lower and company in sent_title_lower:
                            has_security1 = any(keyword in title_lower for keyword in security_keywords)
                            has_security2 = any(keyword in sent_title_lower for keyword in security_keywords)
                            
                            if has_security1 and has_security2:
                                # ë” ìµœì‹  ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
                                if entry_ts(entry) <= entry_ts(sent_entry):
                                    is_duplicate_sent = True
                                    print(f"â­ï¸ ê°™ì€ ê¸°ì—…({company})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë¨: {title[:50]}... (ê¸°ì¡´: {sent_title[:50]}...)")
                                    break
                    if is_duplicate_sent:
                        break
                    
                    # ì´ìŠˆ ë‹¨ìœ„ ì¤‘ë³µ ì²´í¬: (vendor + product + cve + time-window)
                    if ai_judgment and sent_ai_judgment:
                        # CVE ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ê°€ì¥ ì •í™•)
                        cves1 = ai_judgment.get("CVE_IDs", [])
                        cves2 = sent_ai_judgment.get("CVE_IDs", [])
                        
                        # CVE_IDsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        if isinstance(cves1, str):
                            cves1 = [c.strip() for c in cves1.split(",") if c.strip()]
                        if isinstance(cves2, str):
                            cves2 = [c.strip() for c in cves2.split(",") if c.strip()]
                        
                        # ê°™ì€ CVEê°€ ìˆìœ¼ë©´ ê°™ì€ ì´ìŠˆë¡œ ê°„ì£¼ (time-window ì²´í¬)
                        if cves1 and cves2:
                            common_cves = set([c.upper() for c in cves1]) & set([c.upper() for c in cves2])
                            if common_cves:
                                # ê°™ì€ CVEì˜ ê¸°ì‚¬ëŠ” time-window ë‚´ì—ì„œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                                # time-window: 7ì¼ (ê°™ì€ CVEëŠ” 7ì¼ ë‚´ì— í•œ ë²ˆë§Œ ë°œì†¡)
                                time_diff = abs((entry_ts(entry) or 0) - (entry_ts(sent_entry) or 0))
                                if time_diff < 7 * 24 * 3600:  # 7ì¼ = 604800ì´ˆ
                                    is_duplicate_sent = True
                                    cve_list = ", ".join(common_cves)
                                    print(f"â­ï¸ ê°™ì€ CVE({cve_list})ì˜ ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë¨ (ì´ìŠˆ ë‹¨ìœ„ ì¤‘ë³µ): {title[:50]}... (ê¸°ì¡´: {sent_title[:50]}...)")
                                    break
                        
                        # CVEê°€ ì—†ìœ¼ë©´ (vendor + product + í‚¤ì›Œë“œ + time-window)ë¡œ ëŒ€ì²´
                        if not is_duplicate_sent:
                            products1 = ai_judgment.get("products_affected", [])
                            products2 = sent_ai_judgment.get("products_affected", [])
                            
                            # products_affectedê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                            if isinstance(products1, str):
                                products1 = [p.strip() for p in products1.split(",") if p.strip()]
                            if isinstance(products2, str):
                                products2 = [p.strip() for p in products2.split(",") if p.strip()]
                            
                            # ê°™ì€ ì œí’ˆì´ ìˆê³ , ë‘˜ ë‹¤ ì·¨ì•½ì /ì—…ë°ì´íŠ¸ ê´€ë ¨ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                            if products1 and products2:
                                # ì œí’ˆëª… ì •ê·œí™” (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì œê±°)
                                products1_normalized = [p.lower().strip() for p in products1]
                                products2_normalized = [p.lower().strip() for p in products2]
                                
                                # ê³µí†µ ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
                                common_products = set(products1_normalized) & set(products2_normalized)
                                
                                if common_products:
                                    # ë‘˜ ë‹¤ ì·¨ì•½ì /ì—…ë°ì´íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                                    update_keywords = ["íŒ¨ì¹˜", "ì—…ë°ì´íŠ¸", "ë³´ì•ˆ ì—…ë°ì´íŠ¸", "ì·¨ì•½ì ", "íŒ¨ì¹˜ í™”ìš”ì¼", "ë³´ì•ˆ ìœ„í˜‘", "ë³´ì•ˆ ê¶Œê³ ", "cve", "vulnerability"]
                                    has_update1 = any(keyword in title_lower for keyword in update_keywords)
                                    has_update2 = any(keyword in sent_title_lower for keyword in update_keywords)
                                    
                                    # AI íŒë‹¨ì´ ì·¨ì•½ì  ê´€ë ¨ì¸ì§€ë„ í™•ì¸
                                    tags1 = ai_judgment.get("tags", [])
                                    tags2 = sent_ai_judgment.get("tags", [])
                                    is_vuln1 = any("vulnerability" in str(tag).lower() or "cve" in str(tag).lower() for tag in tags1)
                                    is_vuln2 = any("vulnerability" in str(tag).lower() or "cve" in str(tag).lower() for tag in tags2)
                                    
                                    if (has_update1 and has_update2) or (is_vuln1 and is_vuln2):
                                        # ê°™ì€ ì œí’ˆì˜ ê°™ì€ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ì´ìŠˆë¡œ ê°„ì£¼
                                        # time-window: 7ì¼ (ê°™ì€ ì œí’ˆ+í‚¤ì›Œë“œëŠ” 7ì¼ ë‚´ì— í•œ ë²ˆë§Œ ë°œì†¡)
                                        time_diff = abs((entry_ts(entry) or 0) - (entry_ts(sent_entry) or 0))
                                        if time_diff < 7 * 24 * 3600:  # 7ì¼
                                            is_duplicate_sent = True
                                            product_list = ", ".join(common_products)
                                            print(f"â­ï¸ ê°™ì€ ì œí’ˆ({product_list})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë¨ (ì´ìŠˆ ë‹¨ìœ„ ì¤‘ë³µ): {title[:50]}... (ê¸°ì¡´: {sent_title[:50]}...)")
                                            break
                        if is_duplicate_sent:
                            break
                
                if not is_duplicate_sent:
                    print(f"âœ… ë°œì†¡ ê²°ì •: {title[:50]}...")
                    post_one_to_slack(entry, ai_judgment)
                    sent_count += 1
                    # ë°œì†¡ëœ ê¸°ì‚¬ ë§í¬ ì €ì¥
                    if link:
                        normalized_link = normalize_url(link)
                        if normalized_link:
                            sent_links.add(normalized_link)
                    sent_entries.append((entry, ai_judgment))  # ë°œì†¡ëœ ê¸°ì‚¬ì™€ AI íŒë‹¨ ê²°ê³¼ í•¨ê»˜ ì €ì¥
                else:
                    print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ë¡œ ì¸í•´ ë°œì†¡ ê±´ë„ˆëœ€: {title[:50]}...")
            else:
                print(f"â­ï¸ 2ì°¨ íŒë‹¨ ê²°ê³¼: í•„í„°ë§ (ì ìˆ˜: {ai_judgment.get('score', 0)})")
    
    # ìƒíƒœ ì €ì¥
    state["seen"] = list(seen)
    state["seen_titles"] = list(seen_titles)
    state["seen_original_titles"] = seen_original_titles[-1000:]  # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
    state["seen_links"] = list(seen_links)
    state["sent_links"] = list(sent_links)  # ë°œì†¡ëœ ê¸°ì‚¬ ë§í¬ ì €ì¥
    save_state(state)
    
    print(f"\n[ê²°ê³¼ ìš”ì•½]")
    print(f"  - ì´ ê¸°ì‚¬: {len(recent_entries)}ê±´")
    print(f"  - ì¤‘ë³µ ì œê±° í›„: {len(final_entries)}ê±´")
    print(f"  - AI í˜¸ì¶œ: {ai_call_count}ê±´")
    print(f"  - ë°œì†¡: {sent_count}ê±´")
    
    # ë””ë²„ê¹… ë¡œê·¸: ë°œì†¡ëœ ê¸°ì‚¬ ì •ë³´ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
    if sent_entries:
        debug_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_articles": len(recent_entries),
            "sent_count": sent_count,
            "sent_articles": []
        }
        
        for entry, ai_judgment in sent_entries:
            article_data = {
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "ai_judgment": {
                    "decision": ai_judgment.get("decision", "") if ai_judgment else "",
                    "score": ai_judgment.get("score", 0) if ai_judgment else 0,
                    "why": ai_judgment.get("why", []) if ai_judgment else [],
                    "products_affected": ai_judgment.get("products_affected", []) if ai_judgment else [],
                    "tags": ai_judgment.get("tags", []) if ai_judgment else [],
                    "summary_3lines": ai_judgment.get("summary_3lines", "") if ai_judgment else ""
                }
            }
            debug_data["sent_articles"].append(article_data)
        
        debug_file = "debug_sent_entries.json"
        with open(debug_file, "w", encoding="utf-8") as f:
            json.dump(debug_data, f, ensure_ascii=False, indent=2)
        print(f"  - ë””ë²„ê¹… ë¡œê·¸ ì €ì¥: {debug_file}")
    
    return sent_count

# -----------------------------
# main
# -----------------------------
if __name__ == "__main__":
    print("=" * 60)
    print("AI ì¤‘ì‹¬ ë³´ì•ˆ ë‰´ìŠ¤ í•„í„°ë§ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    print(f"RSS í”¼ë“œ: {len(RSS_URLS)}ê°œ")
    print(f"AI Provider: {AI_PROVIDER}")
    print(f"AI íŒë‹¨ í™œì„±í™”: {USE_AI_JUDGMENT}")
    print("=" * 60)
    
    try:
        count = process_articles_ai_driven()
        print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ: {count}ê±´ ë°œì†¡")
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
