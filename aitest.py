"""
AI ì¤‘ì‹¬ ë³´ì•ˆ ë‰´ìŠ¤ í•„í„°ë§ ë° ìŠ¬ë™ ë°œì†¡ ìŠ¤í¬ë¦½íŠ¸

ì£¼ìš” íŠ¹ì§•:
- AIê°€ ì£¼ë„ì ìœ¼ë¡œ ê¸°ì‚¬ë¥¼ íŒë‹¨í•˜ê³  ì„ íƒ
- ì½”ë“œ í•„í„°ë§ ìµœì†Œí™” (ì¤‘ë³µ ì²´í¬ë§Œ)
- AIì˜ decisionì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‹ ë¢°
"""

import os
import json
import time
import re
import requests
import feedparser
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# RSS í”¼ë“œ URL
RSS_URLS = [
    "https://www.boho.or.kr/kr/rss.do?bbsId=B0000133",    
    "https://www.boannews.com/media/news_rss.xml?kind=1",
    "https://www.dailysecu.com/rss/S1N2.xml"
]

STATE_FILE = "state.aitest.json"  # AI í…ŒìŠ¤íŠ¸ìš© ìƒíƒœ íŒŒì¼
SLACK_WEBHOOK = os.environ.get("SLACK_WEBHOOK_URL", "")
if not SLACK_WEBHOOK:
    print("âš ï¸  ê²½ê³ : SLACK_WEBHOOK_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì—ëŠ” ìŠ¬ë™ ë°œì†¡ì´ ê±´ë„ˆëœë‹ˆë‹¤.")

# AI API ì„¤ì •
USE_AI_JUDGMENT = os.environ.get("USE_AI_JUDGMENT", "true").lower() == "true"
AI_PROVIDER_RAW = os.environ.get("AI_PROVIDER", "anthropic")
AI_PROVIDER = AI_PROVIDER_RAW.strip().lower()  # "openai" or "anthropic"

# OpenAI ì„¤ì •
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

# Anthropic (Claude) ì„¤ì •
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
ANTHROPIC_MODEL = os.environ.get("ANTHROPIC_MODEL", "claude-3-5-haiku-20241022")

# AI íŒë‹¨ ê¸°ì¤€ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ
AI_PROMPT_FILE = os.environ.get("AI_PROMPT_FILE", "ai_prompt_simple.txt")

HEADERS = {
    "User-Agent": "MyrealtripSecurityBot/1.0"
}

# -----------------------------
# ìƒíƒœ ê´€ë¦¬
# -----------------------------
def load_state():
    """ìƒíƒœ íŒŒì¼ ë¡œë“œ"""
    if not os.path.exists(STATE_FILE):
        return {
            "seen": [],
            "seen_titles": [],
            "seen_original_titles": [],
            "seen_links": []
        }
    state = json.load(open(STATE_FILE, "r", encoding="utf-8"))
    # ê¸°ì¡´ state íŒŒì¼ì— í•„ë“œê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ˆê¸°í™”
    if "seen_titles" not in state:
        state["seen_titles"] = []
    if "seen_original_titles" not in state:
        state["seen_original_titles"] = []
    if "seen_links" not in state:
        state["seen_links"] = []
    return state

def save_state(state):
    """ìƒíƒœ íŒŒì¼ ì €ì¥"""
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

# -----------------------------
# RSS ìœ í‹¸
# -----------------------------
def entry_uid(e):
    """ê¸°ì‚¬ ê³ ìœ  ID ìƒì„± (link ìš°ì„ )"""
    link = getattr(e, "link", None) or e.get("link")
    if link:
        return link
    return (
        getattr(e, "id", None)
        or getattr(e, "guid", None)
        or e.get("title")
    )

def entry_ts(e):
    """ê¸°ì‚¬ ë°œí–‰ ì‹œê°„ (íƒ€ì„ìŠ¤íƒ¬í”„)"""
    t = getattr(e, "published_parsed", None) or getattr(e, "updated_parsed", None)
    if t:
        return int(time.mktime(t))
    return 0

def fetch_all_recent_entries(max_entries=20):
    """ëª¨ë“  RSS í”¼ë“œì—ì„œ ìµœê·¼ ê¸°ì‚¬ë“¤ì„ ê°€ì ¸ì˜´"""
    all_entries = []
    
    for url in RSS_URLS:
        try:
            r = requests.get(url, headers=HEADERS, timeout=10)
            r.raise_for_status()
            r.encoding = r.apparent_encoding or 'utf-8'
            feed = feedparser.parse(r.text)
            
            if not feed.entries:
                print(f"[DEBUG] RSS í”¼ë“œì— ê¸°ì‚¬ ì—†ìŒ: {url}")
                continue
            
            print(f"[DEBUG] {url}ì—ì„œ {len(feed.entries)}ê±´ ë°œê²¬")
            all_entries.extend(feed.entries)
            
        except Exception as e:
            print(f"[ERROR] fetch failed: {url}, {e}")
            continue
    
    # ì‹œê°„ìˆœ ì •ë ¬
    all_entries.sort(key=entry_ts, reverse=True)
    return all_entries[:max_entries]

def normalize_title(title):
    """ì œëª©ì„ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì²´í¬ì— ì‚¬ìš©"""
    if not title:
        return ""
    normalized = re.sub(r'\s+', '', title.lower())
    normalized = re.sub(r'[^\wê°€-í£]', '', normalized)
    return normalized

def normalize_url(url):
    """URLì„ ì •ê·œí™”í•˜ì—¬ ê°™ì€ URLì„ í™•ì‹¤í•˜ê²Œ ì‹ë³„"""
    if not url:
        return ""
    
    url = url.strip().lower()
    
    # trailing slash ì œê±° (ë‹¨, ë£¨íŠ¸ ê²½ë¡œëŠ” ìœ ì§€)
    if url.endswith('/') and len(url) > 1:
        url = url.rstrip('/')
    
    # URL íŒŒì‹±í•˜ì—¬ query parameter ì •ë¦¬
    try:
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        sorted_query = urlencode(sorted(query_params.items()), doseq=True)
        normalized = urlunparse(parsed._replace(query=sorted_query, fragment=''))
        return normalized
    except Exception:
        pass
    
    return url

def extract_keywords(title):
    """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€, ì˜ë¬¸, ìˆ«ì)"""
    if not title:
        return set()
    
    # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
    korean_words = re.findall(r'[ê°€-í£]+', title)
    # ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
    english_words = re.findall(r'[a-zA-Z]{2,}', title)
    # ìˆ«ì ì¶”ì¶œ
    numbers = re.findall(r'\d+', title)
    
    keywords = set(korean_words + [w.lower() for w in english_words] + numbers)
    return keywords

def is_similar_title(title1, title2, threshold=0.4):
    """ì œëª© ìœ ì‚¬ë„ ì²´í¬ (í‚¤ì›Œë“œ ê¸°ë°˜ Jaccard ìœ ì‚¬ë„)"""
    if not title1 or not title2:
        return False
    
    if title1 == title2:
        return True
    
    keywords1 = extract_keywords(title1)
    keywords2 = extract_keywords(title2)
    
    if not keywords1 or not keywords2:
        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì •ê·œí™”ëœ ë¬¸ìì—´ ê¸¸ì´ ë¹„êµ
        norm1 = normalize_title(title1)
        norm2 = normalize_title(title2)
        if not norm1 or not norm2:
            return False
        # ê¸¸ì´ ì°¨ì´ê°€ 30% ì´ë‚´ë©´ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨
        len_diff = abs(len(norm1) - len(norm2)) / max(len(norm1), len(norm2))
        return len_diff < 0.3
    
    # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
    intersection = keywords1 & keywords2
    union = keywords1 | keywords2
    
    if not union:
        return False
    
    similarity = len(intersection) / len(union)
    return similarity >= threshold

# -----------------------------
# AI í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
# -----------------------------
def load_ai_prompt():
    """AI íŒë‹¨ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì½ì–´ì˜´"""
    prompt_file = AI_PROMPT_FILE
    if os.path.exists(prompt_file):
        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read().strip()
    else:
        print(f"[ERROR] AI í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_file}")
        raise FileNotFoundError(f"AI í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {prompt_file}")

try:
    AI_JUDGMENT_PROMPT = load_ai_prompt()
except FileNotFoundError as e:
    AI_JUDGMENT_PROMPT = None
    if USE_AI_JUDGMENT:
        print(f"[ERROR] {e}")

# -----------------------------
# AI í”„ë¡¬í”„íŠ¸ ìƒì„±
# -----------------------------
def create_ai_prompt(e, task_description=None):
    """ê¸°ì‚¬ ì •ë³´ë¥¼ AIê°€ íŒë‹¨í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    title = e.get("title", "(ì œëª© ì—†ìŒ)")
    link = e.get("link", "")
    summary = e.get("summary", e.get("description", ""))
    published = e.get("published", e.get("updated", ""))
    author = e.get("author", "")
    
    tags = []
    if hasattr(e, "tags") and e.tags:
        tags = [tag.get("term", "") for tag in e.tags if tag.get("term")]
    
    article_info = f"""ì œëª©: {title}
ë§í¬: {link}
ë°œí–‰ì¼: {published}
ì‘ì„±ì: {author if author else "(ì •ë³´ ì—†ìŒ)"}
íƒœê·¸: {', '.join(tags) if tags else "(íƒœê·¸ ì—†ìŒ)"}

ê¸°ì‚¬ ìš”ì•½/ë‚´ìš©:
{summary}"""
    
    if not task_description:
        raise ValueError("í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    korean_instruction = f"""
[ì¤‘ìš”] ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    
    if "[User]" in task_description:
        prompt = task_description.replace(
            "[User]\nEvaluate the following article/advisory/CVE according to the above criteria:\n(Title, date, article content or link)",
            f"[User]\nEvaluate the following article/advisory/CVE according to the above criteria:\n\n{article_info}{korean_instruction}"
        )
    else:
        prompt = f"{task_description}\n\n[User]\nEvaluate the following article/advisory/CVE according to the above criteria:\n\n{article_info}{korean_instruction}"
    
    return prompt

# -----------------------------
# AI íŒë‹¨
# -----------------------------
def judge_with_ai(e, custom_prompt=None):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ë¥¼ íŒë‹¨"""
    if not USE_AI_JUDGMENT:
        print("[WARN] USE_AI_JUDGMENTê°€ Falseë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return None
    
    if AI_JUDGMENT_PROMPT is None:
        print("[ERROR] AI í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # AI_PROVIDER ê°’ í™•ì¸ ë° ì¶œë ¥ (ë§ˆìŠ¤í‚¹ ë°©ì§€ë¥¼ ìœ„í•´ ê¸¸ì´ì™€ ì²« ê¸€ìë§Œ í‘œì‹œ)
    provider_display = f"{AI_PROVIDER[0]}{'*' * (len(AI_PROVIDER) - 2) if len(AI_PROVIDER) > 2 else '*'}{AI_PROVIDER[-1]}" if len(AI_PROVIDER) > 1 else AI_PROVIDER
    print(f"[DEBUG] AI_PROVIDER ê°’: '{provider_display}' (ì‹¤ì œ ê¸¸ì´: {len(AI_PROVIDER)}, ì†Œë¬¸ì ë³€í™˜ í›„: '{AI_PROVIDER}')")
    print(f"[DEBUG] AI_PROVIDERê°€ 'anthropic'ê³¼ ê°™ì€ê°€? {AI_PROVIDER == 'anthropic'}")
    
    # API í‚¤ í™•ì¸
    if AI_PROVIDER == "anthropic":
        if not ANTHROPIC_API_KEY:
            print("[WARN] ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print(f"[DEBUG] AI_PROVIDER: {AI_PROVIDER}, ANTHROPIC_API_KEY ê¸¸ì´: {len(ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else 0}")
            return None
        print(f"[DEBUG] AI Provider: Anthropic, Model: {ANTHROPIC_MODEL}")
    else:
        if not OPENAI_API_KEY:
            print(f"[WARN] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (í˜„ì¬ AI_PROVIDER: '{AI_PROVIDER}')")
            print(f"[DEBUG] AI_PROVIDER: {AI_PROVIDER}, OPENAI_API_KEY ê¸¸ì´: {len(OPENAI_API_KEY) if OPENAI_API_KEY else 0}")
            print(f"[WARN] AI_PROVIDERê°€ 'anthropic'ì´ ì•„ë‹™ë‹ˆë‹¤. GitHub Secretsì—ì„œ AI_PROVIDERë¥¼ 'anthropic'ìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜, Secretsë¥¼ ì œê±°í•˜ì—¬ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            return None
        print(f"[DEBUG] AI Provider: OpenAI, Model: {OPENAI_MODEL}")
    
    try:
        prompt_to_use = custom_prompt or AI_JUDGMENT_PROMPT
        if prompt_to_use is None:
            print("[ERROR] í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        prompt = create_ai_prompt(e, prompt_to_use)
        
        print(f"[DEBUG] í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)} ë¬¸ì)")
        
        # Anthropic (Claude) API í˜¸ì¶œ
        if AI_PROVIDER == "anthropic":
            headers = {
                "x-api-key": ANTHROPIC_API_KEY,
                "anthropic-version": "2023-06-01",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": ANTHROPIC_MODEL,
                "max_tokens": 4096,
                "system": "ë‹¹ì‹ ì€ ë³´ì•ˆ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”. ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }
            
            print(f"[DEBUG] Anthropic API í˜¸ì¶œ ì¤‘...")
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            ai_response = result["content"][0]["text"]
            print(f"[DEBUG] API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(ai_response)} ë¬¸ì)")
        
        # OpenAI API í˜¸ì¶œ
        else:
            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": OPENAI_MODEL,
                "messages": [
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë³´ì•ˆ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”. ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 1000
            }
            
            print(f"[DEBUG] OpenAI API í˜¸ì¶œ ì¤‘...")
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            ai_response = result["choices"][0]["message"]["content"]
            print(f"[DEBUG] API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(ai_response)} ë¬¸ì)")
        
        # JSON íŒŒì‹±
        try:
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            cleaned_response = re.sub(r'```json\s*', '', ai_response)
            cleaned_response = re.sub(r'```\s*', '', cleaned_response)
            cleaned_response = cleaned_response.strip()
            
            # JSON ê°ì²´ ì°¾ê¸°
            brace_count = 0
            start_idx = -1
            json_str = None
            for i, char in enumerate(cleaned_response):
                if char == '{':
                    if start_idx == -1:
                        start_idx = i
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0 and start_idx != -1:
                        json_str = cleaned_response[start_idx:i+1]
                        break
            
            if json_str:
                # JSON ë¬¸ìì—´ ë‚´ë¶€ì˜ ì¤„ë°”ê¿ˆ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                def fix_newlines_in_json_strings(text):
                    """JSON ë¬¸ìì—´ ë‚´ë¶€ì˜ ì¤„ë°”ê¿ˆì„ ì´ìŠ¤ì¼€ì´í”„"""
                    result = []
                    i = 0
                    in_string = False
                    escape_next = False
                    
                    while i < len(text):
                        char = text[i]
                        
                        if escape_next:
                            result.append(char)
                            escape_next = False
                        elif char == '\\':
                            result.append(char)
                            escape_next = True
                        elif char == '"' and not escape_next:
                            in_string = not in_string
                            result.append(char)
                        elif in_string and char == '\n':
                            result.append('\\n')
                        elif in_string and char == '\r':
                            result.append('\\r')
                        else:
                            result.append(char)
                        
                        i += 1
                    
                    return ''.join(result)
                
                json_str = fix_newlines_in_json_strings(json_str)
                judgment = json.loads(json_str)
                judgment["_raw_response"] = ai_response
            else:
                print(f"[WARN] JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì‘ë‹µ:\n{ai_response[:500]}")
                judgment = {"raw_response": ai_response}
        except (json.JSONDecodeError, ValueError) as e:
            print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"[WARN] ì›ë³¸ ì‘ë‹µ:\n{ai_response[:500]}")
            judgment = {"raw_response": ai_response}
        
        return judgment
        
    except Exception as e:
        print(f"[ERROR] AI judgment failed: {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
                print(f"[ERROR] API Error Detail: {error_detail}")
            except:
                try:
                    print(f"[ERROR] Response Status: {e.response.status_code}")
                    print(f"[ERROR] Response Text: {e.response.text[:500]}")
                except:
                    pass
        return None

# -----------------------------
# AI ì¤‘ì‹¬ ë°œì†¡ ê²°ì •
# -----------------------------
def should_send_article_ai_driven(ai_judgment, entry=None):
    """
    AI ì¤‘ì‹¬ ë°œì†¡ ê²°ì • í•¨ìˆ˜
    - AIì˜ decisionì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‹ ë¢°
    - ìµœì†Œí•œì˜ ê²€ì¦ë§Œ ìˆ˜í–‰
    """
    if not ai_judgment:
        print("[INFO] AI íŒë‹¨ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ì ìœ¼ë¡œ ë°œì†¡í•˜ì§€ ì•ŠìŒ")
        return False
    
    # decision í•„ë“œ í™•ì¸
    if "decision" not in ai_judgment:
        print("[WARN] decision í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤. JSON íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ íŒŒì¼ í˜•ì‹ì´ ì•„ë‹Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        return False
    
    decision = ai_judgment.get("decision", "SKIP")
    score = ai_judgment.get("score", 0)
    
    # AI íŒë‹¨ ê·¼ê±° ìƒì„¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    why_list = ai_judgment.get("why", [])
    why_text = " | ".join(why_list) if isinstance(why_list, list) else str(why_list)
    products_affected = ai_judgment.get("products_affected", [])
    products_text = ", ".join(products_affected) if isinstance(products_affected, list) else str(products_affected)
    tags = ai_judgment.get("tags", [])
    tags_text = ", ".join(tags) if isinstance(tags, list) else str(tags)
    
    print(f"[AI íŒë‹¨ ìƒì„¸]")
    print(f"  - Decision: {decision}")
    print(f"  - Score: {score}/100")
    if why_text:
        print(f"  - Why: {why_text}")
    if products_text:
        print(f"  - Products: {products_text}")
    if tags_text:
        print(f"  - Tags: {tags_text}")
    
    # AIì˜ decisionì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ë”°ë¦„
    if decision == "SCRAPE":
        print(f"[INFO] AI íŒë‹¨: SCRAPE (ì ìˆ˜: {score}) â†’ ë°œì†¡")
        return True
    elif decision == "WATCHLIST":
        print(f"[INFO] AI íŒë‹¨: WATCHLIST (ì ìˆ˜: {score}) â†’ ë°œì†¡")
        return True
    elif decision == "SKIP":
        print(f"[INFO] AI íŒë‹¨: SKIP (ì ìˆ˜: {score}) â†’ í•„í„°ë§")
        return False
    else:
        print(f"[WARN] ì•Œ ìˆ˜ ì—†ëŠ” decision: {decision} â†’ í•„í„°ë§")
        return False

# -----------------------------
# Slack ë°œì†¡
# -----------------------------
def post_one_to_slack(e, ai_judgment=None):
    """ìŠ¬ë™ì— ê¸°ì‚¬ ë°œì†¡"""
    if not SLACK_WEBHOOK:
        print("âš ï¸  SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ìŠ¬ë™ ë°œì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print(f"   ê¸°ì‚¬: {e.get('title', '')[:50]}...")
        return
    
    title = e.get("title", "(no title)")
    link = e.get("link", "")
    published = e.get("published", e.get("updated", ""))
    
    # ìœ„í—˜ë„ì— ë”°ë¥¸ ìƒ‰ìƒ/ì´ëª¨ì§€ ê²°ì • (score ê¸°ë°˜) - í”„ë¡¬í”„íŠ¸ ê¸°ì¤€ê³¼ í†µì¼
    risk_indicator = ""
    if ai_judgment and "score" in ai_judgment:
        score = ai_judgment.get("score", 0)
        if score >= 81:
            risk_indicator = "ğŸ”´ *[ë†’ì€ ìœ„í—˜]*"
        elif score >= 51:
            risk_indicator = "ğŸŸ¡ *[ì¤‘ê°„ ìœ„í—˜]*"
        else:
            risk_indicator = "ğŸŸ¢ *[ë‚®ì€ ìœ„í—˜]*"
    elif ai_judgment and "severity" in ai_judgment:
        severity = ai_judgment.get("severity", "Unknown")
        if severity in ["Critical", "High"]:
            risk_indicator = "ğŸ”´ *[ë†’ì€ ìœ„í—˜]*"
        elif severity == "Medium":
            risk_indicator = "ğŸŸ¡ *[ì¤‘ê°„ ìœ„í—˜]*"
        else:
            risk_indicator = "ğŸŸ¢ *[ë‚®ì€ ìœ„í—˜]*"
    
    # ìœ„í—˜ë„ bar ì´ëª¨ì§€ - í”„ë¡¬í”„íŠ¸ ê¸°ì¤€ê³¼ í†µì¼
    risk_bar_emoji = ""
    if ai_judgment and "score" in ai_judgment:
        score = ai_judgment.get("score", 0)
        if score >= 81:
            risk_bar_emoji = "ğŸ”´"
        elif score >= 51:
            risk_bar_emoji = "ğŸŸ¡"
        else:
            risk_bar_emoji = "ğŸŸ¢"
    elif ai_judgment and "severity" in ai_judgment:
        severity = ai_judgment.get("severity", "Unknown")
        if severity in ["Critical", "High"]:
            risk_bar_emoji = "ğŸ”´"
        elif severity == "Medium":
            risk_bar_emoji = "ğŸŸ¡"
        else:
            risk_bar_emoji = "ğŸŸ¢"
    
    blocks = [
        {
            "type": "header",
            "text": {"type": "plain_text", "text": "ğŸ”” ë³´ì•ˆë‰´ìŠ¤ ì•Œë¦¼"}
        },
    ]
    
    # AI íŒë‹¨ ì ìˆ˜ ì¶”ê°€
    score_text = ""
    if ai_judgment and "score" in ai_judgment:
        score = ai_judgment.get("score", 0)
        decision = ai_judgment.get("decision", "UNKNOWN")
        score_text = f"\nğŸ“Š AI íŒë‹¨: {decision} (ì ìˆ˜: {score}/100)"
    
    title_section_text = f"*<{link}|{title}>*\nğŸ“… {published}{score_text}"
    if risk_indicator:
        title_section_text = f"{risk_indicator}\n{title_section_text}"
    
    if risk_bar_emoji:
        title_with_bar = f"{risk_bar_emoji} {title_section_text}"
        blocks.append({
            "type": "section",
            "text": {"type": "mrkdwn", "text": title_with_bar}
        })
    else:
        blocks.append({
            "type": "section",
            "text": {"type": "mrkdwn", "text": title_section_text}
        })
    
    # AI íŒë‹¨ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê¸°ì‚¬ ìš”ì•½ ì¶”ê°€
    if ai_judgment:
        if ai_judgment.get("summary_3lines"):
            blocks.append({
                "type": "divider"
            })
            summary_text = ai_judgment['summary_3lines']
            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": summary_text}
            })
    
    resp = requests.post(
        SLACK_WEBHOOK,
        json={"blocks": blocks},
        timeout=10,
    )
    resp.raise_for_status()

# -----------------------------
# ë©”ì¸ ì²˜ë¦¬ (AI ì¤‘ì‹¬)
# -----------------------------
def process_articles_ai_driven():
    """
    AI ì¤‘ì‹¬ ê¸°ì‚¬ ì²˜ë¦¬
    - ì¤‘ë³µ ì²´í¬ë§Œ ì½”ë“œë¡œ ìˆ˜í–‰
    - AI íŒë‹¨ì„ ìµœëŒ€í•œ ì‹ ë¢°
    """
    state = load_state()
    seen = set(state.get("seen", []))
    seen_titles = set(state.get("seen_titles", []))
    seen_original_titles = state.get("seen_original_titles", [])
    seen_links = set(state.get("seen_links", []))
    
    # ìµœê·¼ ê¸°ì‚¬ë“¤ ê°€ì ¸ì˜¤ê¸°
    recent_entries = fetch_all_recent_entries(max_entries=20)
    
    if not recent_entries:
        print("âŒ No RSS entries found.")
        return 0
    
    print(f"\n[INFO] ì´ {len(recent_entries)}ê±´ì˜ ê¸°ì‚¬ ë°œê²¬")
    
    # 1ë‹¨ê³„: ì¤‘ë³µ ì²´í¬ (ì½”ë“œ ê¸°ë°˜)
    candidate_entries = []
    
    for entry in recent_entries:
        link = entry.get("link", "")
        title = entry.get("title", "")
        
        # ë§í¬ URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        if link:
            normalized_link = normalize_url(link)
            if normalized_link and normalized_link in seen_links:
                print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ ë§í¬ ë°œê²¬: {title[:50]}...")
                continue
        
        uid = entry_uid(entry)
        normalized_title = normalize_title(title)
        
        # UID ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        if uid and uid in seen:
            print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ UID ë°œê²¬: {title[:50]}...")
            continue
        
        # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        if normalized_title and normalized_title in seen_titles:
            print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ ì œëª© ë°œê²¬: {title[:50]}...")
            continue
        
        # ìœ ì‚¬ ì œëª© ì²´í¬
        is_duplicate = False
        for seen_title in seen_original_titles:
            if is_similar_title(title, seen_title):
                print(f"â­ï¸ ìœ ì‚¬í•œ ê¸°ì‚¬ ì œëª© ë°œê²¬: {title[:50]}... (ê¸°ì¡´: {seen_title[:50]}...)")
                is_duplicate = True
                break
        
        if is_duplicate:
            continue
        
        candidate_entries.append(entry)
    
    print(f"[INFO] ì¤‘ë³µ ì œê±° í›„ {len(candidate_entries)}ê±´ì˜ ì‹ ê·œ ê¸°ì‚¬")
    
    if not candidate_entries:
        print("âœ… ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    # 2ë‹¨ê³„: ê°™ì€ ë°°ì¹˜ ë‚´ ìœ ì‚¬ ì œëª© ì¤‘ë³µ ì œê±° (ìµœì‹  ê²ƒë§Œ ì„ íƒ)
    final_entries = []
    for i, entry in enumerate(candidate_entries):
        title = entry.get("title", "")
        is_duplicate_in_batch = False
        
        for j, other_entry in enumerate(candidate_entries):
            if i >= j:
                continue
            other_title = other_entry.get("title", "")
            if title == other_title:
                continue
            if is_similar_title(title, other_title):
                # ë” ìµœì‹  ê¸°ì‚¬ ì„ íƒ
                if entry_ts(entry) < entry_ts(other_entry):
                    is_duplicate_in_batch = True
                    break
        
        # ì¶”ê°€: ê°™ì€ ê¸°ì—…ëª… + ìœ ì‚¬í•œ ë³´ì•ˆ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        if not is_duplicate_in_batch:
            title_lower = title.lower()
            # í•œêµ­ ê¸°ì—…ëª… íŒ¨í„´ ì¶”ì¶œ
            korean_companies = ["êµì›", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "ì‚¼ì„±", "LG", "SK", "í˜„ëŒ€", "ê¸°ì•„", "ë¡¯ë°", "í•œí™”", "ë‘ì‚°", "í¬ìŠ¤ì½”", "KT", "ì‹ í•œ", "ë¼ì¸", "ì¿ íŒ¡"]
            security_keywords = ["í•´í‚¹", "ëœì„¬ì›¨ì–´", "ì •ë³´ìœ ì¶œ", "ì¹¨í•´", "ê³µê²©", "ì‚¬ê³ ", "ìœ ì¶œ"]
            
            for company in korean_companies:
                if company in title_lower:
                    # ê°™ì€ ê¸°ì—…ëª…ì´ ìˆëŠ” ë‹¤ë¥¸ ê¸°ì‚¬ ì°¾ê¸°
                    for j, other_entry in enumerate(candidate_entries):
                        if i >= j:
                            continue
                        other_title = other_entry.get("title", "")
                        other_title_lower = other_title.lower()
                        
                        # ê°™ì€ ê¸°ì—…ëª… + ë³´ì•ˆ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µ ê°€ëŠ¥ì„±
                        if company in other_title_lower:
                            # ë‘˜ ë‹¤ ë³´ì•ˆ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
                            has_security1 = any(keyword in title_lower for keyword in security_keywords)
                            has_security2 = any(keyword in other_title_lower for keyword in security_keywords)
                            
                            if has_security1 and has_security2:
                                # ë” ìµœì‹  ê¸°ì‚¬ ì„ íƒ
                                if entry_ts(entry) < entry_ts(other_entry):
                                    is_duplicate_in_batch = True
                                    print(f"â­ï¸ ê°™ì€ ê¸°ì—…({company})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ê¸°ì‚¬ ì¤‘ë³µ ë°œê²¬: {title[:50]}... (ìµœì‹  ê¸°ì‚¬ ì„ íƒ)")
                                    break
                    if is_duplicate_in_batch:
                        break
            
            # ì œí’ˆëª… + ë³´ì•ˆ ì—…ë°ì´íŠ¸ í‚¤ì›Œë“œ ì¤‘ë³µ ì²´í¬ (AI íŒë‹¨ ì „ì´ë¯€ë¡œ ì œëª© ê¸°ë°˜)
            # ì£¼ì˜: ì´ ë¶€ë¶„ì€ AI íŒë‹¨ ì „ì´ë¯€ë¡œ ì œëª© í‚¤ì›Œë“œ ë§¤ì¹­ ì‚¬ìš©
            # ë°œì†¡ ì „ ì¤‘ë³µ ì²´í¬ì—ì„œëŠ” AIì˜ products_affected í•„ë“œë¥¼ í™œìš©í•¨
            if not is_duplicate_in_batch:
                product_keywords = {
                    "windows": ["windows", "ìœˆë„ìš°", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "ms", "microsoft"],
                    "office": ["office", "ì˜¤í”¼ìŠ¤", "office 365", "microsoft office"],
                    "adobe reader": ["adobe reader", "ì–´ë„ë¹„ ë¦¬ë”", "adobe acrobat reader"],
                    "fortigate": ["fortigate", "í¬í‹°ê²Œì´íŠ¸", "fortios"]
                }
                update_keywords = ["íŒ¨ì¹˜", "ì—…ë°ì´íŠ¸", "ë³´ì•ˆ ì—…ë°ì´íŠ¸", "ì·¨ì•½ì ", "íŒ¨ì¹˜ í™”ìš”ì¼", "ë³´ì•ˆ ìœ„í˜‘", "ë³´ì•ˆ ê¶Œê³ ", "cve"]
                
                for product, product_names in product_keywords.items():
                    if any(name in title_lower for name in product_names):
                        # ê°™ì€ ì œí’ˆì˜ ë‹¤ë¥¸ ê¸°ì‚¬ ì°¾ê¸°
                        for j, other_entry in enumerate(candidate_entries):
                            if i >= j:
                                continue
                            other_title = other_entry.get("title", "")
                            other_title_lower = other_title.lower()
                            
                            # ê°™ì€ ì œí’ˆëª…ì´ ìˆëŠ”ì§€ í™•ì¸
                            if any(name in other_title_lower for name in product_names):
                                # ë‘˜ ë‹¤ ì—…ë°ì´íŠ¸/íŒ¨ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ë³µ ê°€ëŠ¥ì„±
                                has_update1 = any(keyword in title_lower for keyword in update_keywords)
                                has_update2 = any(keyword in other_title_lower for keyword in update_keywords)
                                
                                if has_update1 and has_update2:
                                    # ê°™ì€ ì œí’ˆì˜ ê°™ì€ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ì´ìŠˆë¡œ ê°„ì£¼
                                    # ë” ìµœì‹  ê¸°ì‚¬ ì„ íƒ
                                    if entry_ts(entry) < entry_ts(other_entry):
                                        is_duplicate_in_batch = True
                                        print(f"â­ï¸ ê°™ì€ ì œí’ˆ({product})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ê¸°ì‚¬ ì¤‘ë³µ ë°œê²¬: {title[:50]}... (ìµœì‹  ê¸°ì‚¬ ì„ íƒ)")
                                        break
                        if is_duplicate_in_batch:
                            break
                    if is_duplicate_in_batch:
                        break
        
        if not is_duplicate_in_batch:
            final_entries.append(entry)
    
    print(f"[INFO] ë°°ì¹˜ ë‚´ ì¤‘ë³µ ì œê±° í›„ {len(final_entries)}ê±´ì˜ ê¸°ì‚¬")
    
    # 3ë‹¨ê³„: AI íŒë‹¨ (AI ì£¼ë„)
    new_entries = []
    ai_call_count = 0
    sent_count = 0
    sent_entries = []  # ë°œì†¡ëœ ê¸°ì‚¬ ëª©ë¡ (ì¤‘ë³µ ì²´í¬ìš©) - (entry, ai_judgment) íŠœí”Œ ì €ì¥
    
    for entry in final_entries:
        title = entry.get("title", "")
        link = entry.get("link", "")
        uid = entry_uid(entry)
        normalized_title = normalize_title(title)
        
        print(f"\n[AI íŒë‹¨ ì‹œì‘] {title[:60]}...")
        
        # AI íŒë‹¨
        ai_judgment = judge_with_ai(entry)
        ai_call_count += 1
        
        if not ai_judgment:
            print(f"[WARN] AI íŒë‹¨ ì‹¤íŒ¨: {title[:50]}...")
            continue
        
        # AI ì¤‘ì‹¬ ë°œì†¡ ê²°ì •
        if should_send_article_ai_driven(ai_judgment, entry):
            # ë°œì†¡ ì „ ì¤‘ë³µ ì²´í¬: ê°™ì€ ë°°ì¹˜ ë‚´ì—ì„œ ì´ë¯¸ ë°œì†¡ëœ ìœ ì‚¬í•œ ê¸°ì‚¬ê°€ ìˆëŠ”ì§€ í™•ì¸
            is_duplicate_sent = False
            title_lower = title.lower()
            
            for sent_entry_data in sent_entries:
                sent_entry, sent_ai_judgment = sent_entry_data
                sent_title = sent_entry.get("title", "")
                sent_title_lower = sent_title.lower()
                
                # ì •í™•íˆ ê°™ì€ ì œëª©
                if title == sent_title:
                    is_duplicate_sent = True
                    print(f"â­ï¸ ì´ë¯¸ ë°œì†¡ëœ ë™ì¼ ì œëª© ê¸°ì‚¬ ë°œê²¬: {title[:50]}...")
                    break
                
                # ìœ ì‚¬í•œ ì œëª©
                if is_similar_title(title, sent_title, threshold=0.3):
                    is_duplicate_sent = True
                    print(f"â­ï¸ ì´ë¯¸ ë°œì†¡ëœ ìœ ì‚¬ ì œëª© ê¸°ì‚¬ ë°œê²¬: {title[:50]}... (ê¸°ì¡´: {sent_title[:50]}...)")
                    break
                
                # ê°™ì€ ê¸°ì—…ëª… + ë³´ì•ˆ í‚¤ì›Œë“œ
                korean_companies = ["êµì›", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "ì‚¼ì„±", "LG", "SK", "í˜„ëŒ€", "ê¸°ì•„", "ë¡¯ë°", "í•œí™”", "ë‘ì‚°", "í¬ìŠ¤ì½”", "KT", "ì‹ í•œ", "ë¼ì¸", "ì¿ íŒ¡"]
                security_keywords = ["í•´í‚¹", "ëœì„¬ì›¨ì–´", "ì •ë³´ìœ ì¶œ", "ì¹¨í•´", "ê³µê²©", "ì‚¬ê³ ", "ìœ ì¶œ"]
                
                for company in korean_companies:
                    if company in title_lower and company in sent_title_lower:
                        has_security1 = any(keyword in title_lower for keyword in security_keywords)
                        has_security2 = any(keyword in sent_title_lower for keyword in security_keywords)
                        
                        if has_security1 and has_security2:
                            # ë” ìµœì‹  ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
                            if entry_ts(entry) <= entry_ts(sent_entry):
                                is_duplicate_sent = True
                                print(f"â­ï¸ ê°™ì€ ê¸°ì—…({company})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë¨: {title[:50]}... (ê¸°ì¡´: {sent_title[:50]}...)")
                                break
                if is_duplicate_sent:
                    break
                
                # AIì˜ products_affected í•„ë“œë¥¼ í™œìš©í•œ ì¤‘ë³µ ì²´í¬ (ì œí’ˆëª… í•˜ë“œì½”ë”© ì œê±°)
                if ai_judgment and sent_ai_judgment:
                    products1 = ai_judgment.get("products_affected", [])
                    products2 = sent_ai_judgment.get("products_affected", [])
                    
                    # products_affectedê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    if isinstance(products1, str):
                        products1 = [p.strip() for p in products1.split(",") if p.strip()]
                    if isinstance(products2, str):
                        products2 = [p.strip() for p in products2.split(",") if p.strip()]
                    
                    # ê°™ì€ ì œí’ˆì´ ìˆê³ , ë‘˜ ë‹¤ ì·¨ì•½ì /ì—…ë°ì´íŠ¸ ê´€ë ¨ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                    if products1 and products2:
                        # ì œí’ˆëª… ì •ê·œí™” (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì œê±°)
                        products1_normalized = [p.lower().strip() for p in products1]
                        products2_normalized = [p.lower().strip() for p in products2]
                        
                        # ê³µí†µ ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
                        common_products = set(products1_normalized) & set(products2_normalized)
                        
                        if common_products:
                            # ë‘˜ ë‹¤ ì·¨ì•½ì /ì—…ë°ì´íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                            update_keywords = ["íŒ¨ì¹˜", "ì—…ë°ì´íŠ¸", "ë³´ì•ˆ ì—…ë°ì´íŠ¸", "ì·¨ì•½ì ", "íŒ¨ì¹˜ í™”ìš”ì¼", "ë³´ì•ˆ ìœ„í˜‘", "ë³´ì•ˆ ê¶Œê³ ", "cve", "vulnerability"]
                            has_update1 = any(keyword in title_lower for keyword in update_keywords)
                            has_update2 = any(keyword in sent_title_lower for keyword in update_keywords)
                            
                            # AI íŒë‹¨ì´ ì·¨ì•½ì  ê´€ë ¨ì¸ì§€ë„ í™•ì¸
                            tags1 = ai_judgment.get("tags", [])
                            tags2 = sent_ai_judgment.get("tags", [])
                            is_vuln1 = any("vulnerability" in str(tag).lower() or "cve" in str(tag).lower() for tag in tags1)
                            is_vuln2 = any("vulnerability" in str(tag).lower() or "cve" in str(tag).lower() for tag in tags2)
                            
                            if (has_update1 and has_update2) or (is_vuln1 and is_vuln2):
                                # ê°™ì€ ì œí’ˆì˜ ê°™ì€ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ì´ìŠˆë¡œ ê°„ì£¼
                                # ë” ìµœì‹  ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
                                if entry_ts(entry) <= entry_ts(sent_entry):
                                    is_duplicate_sent = True
                                    product_list = ", ".join(common_products)
                                    print(f"â­ï¸ ê°™ì€ ì œí’ˆ({product_list})ì˜ ìœ ì‚¬í•œ ë³´ì•ˆ ì—…ë°ì´íŠ¸ ê¸°ì‚¬ê°€ ì´ë¯¸ ë°œì†¡ë¨: {title[:50]}... (ê¸°ì¡´: {sent_title[:50]}...)")
                                    break
                if is_duplicate_sent:
                    break
            
            if not is_duplicate_sent:
                print(f"âœ… ë°œì†¡ ê²°ì •: {title[:50]}...")
                post_one_to_slack(entry, ai_judgment)
                sent_count += 1
                sent_entries.append((entry, ai_judgment))  # ë°œì†¡ëœ ê¸°ì‚¬ì™€ AI íŒë‹¨ ê²°ê³¼ í•¨ê»˜ ì €ì¥
            else:
                print(f"â­ï¸ ì¤‘ë³µ ê¸°ì‚¬ë¡œ ì¸í•´ ë°œì†¡ ê±´ë„ˆëœ€: {title[:50]}...")
        else:
            print(f"â­ï¸ í•„í„°ë§: {title[:50]}...")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        if uid:
            seen.add(uid)
        if normalized_title:
            seen_titles.add(normalized_title)
        if title:
            seen_original_titles.append(title)
        if link:
            normalized_link = normalize_url(link)
            if normalized_link:
                seen_links.add(normalized_link)
        
        new_entries.append({
            "uid": uid or normalized_title or title,
            "title": title,
            "link": link
        })
    
    # ìƒíƒœ ì €ì¥
    state["seen"] = list(seen)
    state["seen_titles"] = list(seen_titles)
    state["seen_original_titles"] = seen_original_titles[-1000:]  # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
    state["seen_links"] = list(seen_links)
    save_state(state)
    
    print(f"\n[ê²°ê³¼ ìš”ì•½]")
    print(f"  - ì´ ê¸°ì‚¬: {len(recent_entries)}ê±´")
    print(f"  - ì¤‘ë³µ ì œê±° í›„: {len(final_entries)}ê±´")
    print(f"  - AI í˜¸ì¶œ: {ai_call_count}ê±´")
    print(f"  - ë°œì†¡: {sent_count}ê±´")
    
    # ë””ë²„ê¹… ë¡œê·¸: ë°œì†¡ëœ ê¸°ì‚¬ ì •ë³´ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
    if sent_entries:
        debug_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_articles": len(recent_entries),
            "sent_count": sent_count,
            "sent_articles": []
        }
        
        for entry, ai_judgment in sent_entries:
            article_data = {
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "ai_judgment": {
                    "decision": ai_judgment.get("decision", "") if ai_judgment else "",
                    "score": ai_judgment.get("score", 0) if ai_judgment else 0,
                    "why": ai_judgment.get("why", []) if ai_judgment else [],
                    "products_affected": ai_judgment.get("products_affected", []) if ai_judgment else [],
                    "tags": ai_judgment.get("tags", []) if ai_judgment else [],
                    "summary_3lines": ai_judgment.get("summary_3lines", "") if ai_judgment else ""
                }
            }
            debug_data["sent_articles"].append(article_data)
        
        debug_file = "debug_sent_entries.json"
        with open(debug_file, "w", encoding="utf-8") as f:
            json.dump(debug_data, f, ensure_ascii=False, indent=2)
        print(f"  - ë””ë²„ê¹… ë¡œê·¸ ì €ì¥: {debug_file}")
    
    return sent_count

# -----------------------------
# main
# -----------------------------
if __name__ == "__main__":
    print("=" * 60)
    print("AI ì¤‘ì‹¬ ë³´ì•ˆ ë‰´ìŠ¤ í•„í„°ë§ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    print(f"RSS í”¼ë“œ: {len(RSS_URLS)}ê°œ")
    print(f"AI Provider: {AI_PROVIDER}")
    print(f"AI íŒë‹¨ í™œì„±í™”: {USE_AI_JUDGMENT}")
    print("=" * 60)
    
    try:
        count = process_articles_ai_driven()
        print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ: {count}ê±´ ë°œì†¡")
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
